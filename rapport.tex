\documentclass[12pt, french]{report}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{1}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage[a4paper]{geometry}
\usepackage{babel}
\usepackage{wrapfig}
\usepackage[figurename=Fig.]{caption}
\usepackage{subfig}

%\usepackage[document]{ragged2e} %for aligning text

\usepackage{amsmath} %Add matrices

\usepackage{longtable} %for creating long table

\usepackage{makecell}%using for thead

\usepackage{enumitem}%for roman list 

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\usepackage[nottoc]{tocbibind}%Automatic approach to add \listoffigures and \listoftables in table of contents

\usepackage{color}   %May be necessary if you want to color links

%Customize chapter format
\usepackage{titlesec, blindtext}
\definecolor{gray75}{gray}{0.75}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{|}\hsp}{0pt}{\Huge\bfseries}
%End customize chapter format


\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=blue,  %choose some color if you want links to stand out
}


\usepackage{quoting,xparse}

\NewDocumentCommand{\bywhom}{m}{% the Bourbaki trick
	{\nobreak\hfill\penalty50\hskip1em\null\nobreak
		\hfill\mbox{\normalfont(#1)}%
		\parfillskip=0pt \finalhyphendemerits=0 \par}%
}

\NewDocumentEnvironment{pquotation}{m}
{\begin{quoting}[
		indentfirst=true,
		leftmargin=\parindent,
		rightmargin=\parindent]\itshape}
	{\bywhom{#1}\end{quoting}}

\usepackage{natbib}

\begin{document}
\renewcommand{\labelitemi}{$\bullet$}
	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.4mm}} % Defines a new command for the horizontal lines, change thickness here
		
		\center % Center everything on the page
		
		%----------------------------------------------------------------------------------------
		%	HEADING SECTIONS
		%----------------------------------------------------------------------------------------
		
		\textsc{\LARGE Université de Montréal}\\[1cm] % Name of your university/college
		\textsc{\Large Faculté des arts et des sciences}\\[0.5cm] % Major heading such as course name
		\textsc{\large Département d’informatique et de recherche opérationnelle (DIRO) }\\[0.3cm] % Minor heading such as course title
		
		%----------------------------------------------------------------------------------------
		%	TITLE SECTION
		%----------------------------------------------------------------------------------------
		
		\HRule \\[0.4cm]
		{ \huge Rapport de stage \\ \bfseries Évaluation du risque de retour à la maison } \\[0.4cm] % Title of your document
		\HRule \\[1.5cm]
		
		%----------------------------------------------------------------------------------------
		%	AUTHOR SECTION
		%----------------------------------------------------------------------------------------
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				\emph{Auteur:}\\
				Vilon \textsc{Saint-Fleurose} % Your name
				\\MSc, informatique \\Université de Montréal
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				\emph{Directeur de recherche:} \\
				Dr. Michalis \textsc{Famelis} % Supervisor's Name
				\\Professeur adjoint \\Université de Montréal
			\end{flushright}
		\end{minipage}\\[1cm]
			~
			\begin{minipage}{0.4\textwidth}
				\begin{center} \large
					\emph{Superviseur:} \\
					 Nicolas \textsc{Coallier} % Supervisor's Name
					\\Vice-Président Exécutif, TIC \\ML+
				\end{center}
			\end{minipage}\\[1cm]
		
		% If you don't want a supervisor, uncomment the two lines below and remove the section above
		%\Large \emph{Author:}\\
		%John \textsc{Smith}\\[3cm] % Your name
		
		%----------------------------------------------------------------------------------------
		%	DATE SECTION
		%----------------------------------------------------------------------------------------
		
		{\large \today}\\[1cm] % Date, change the \today to a set date if you want to be precise
		
		%----------------------------------------------------------------------------------------
		%	LOGO SECTION
		%----------------------------------------------------------------------------------------
		
		\includegraphics[width=3cm, height=1cm]{images/logo.png} % Include a department/university logo - this will require the graphicx package
		
		%----------------------------------------------------------------------------------------
		
		\vfill % Fill the rest of the page with whitespace
		
	\end{titlepage}
		
\tableofcontents
\newpage	
\addcontentsline{toc}{chapter}{Remerciements}
\chapter*{Remerciements}
Je veux commencer d'abord par remercier le Grand Dieu Tout-Puissant, le Créateur de l'univers, des cieux et de la terre qui m'a donné la vie, la santé, les opportunités et tout ce dont j'avais besoin pour faire cette grande et belle étude à l'université de Montréal. Il a rendu toutes choses possibles en ma faveur, moi qui suis pécheur et désobéissant; immérité de toutes ces grâces. Il m'accompagnait toujours dans les moments les plus difficiles de ma vie, Il ne m'a jamais laissé seul; surtout dans les moments où je devais payer les frais de scolarité qui étaient si énormes et impossibles à payer de mon propre compte. A un Dieu si merveilleux et si bon, je Lui dois beaucoup de reconnaissance.  \\
	
	Je remercie aussi ma femme qui m'a beaucoup supporté pendant plus de deux années d'études. Elle n'a jamais murmuré, ni découragé quand nous devions passer par des moments difficiles de notre vie conjugale à cause de ces études. Elle a mis toutes ses ressources disponibles pour entretenir la famille et payer mes études quand j’étais moi-même dans l’impossibilité de travailler. Vraiment, ma femme est une bénédiction dans ma vie, un cadeau venant de Dieu. Je t’aime ma chérie. \\
	
	Je tiens à remercier le professeur Michalis Famelis d’avoir accepté être mon directeur de recherche et supervisé ce stage, il est toujours là pour m’encourager et me pousser vers l’avant. Il répond toujours présent à tous mes appels, il est toujours disponible pour me rencontrer, me parler et me conseiller ; même en dehors du cadre universitaire.  \\
	
	Je remercie Nicolas Coallier et toute l’équipe ML+ qui ont accepté que je sois leur stagiaire, ils ont placé  leur confiance en moi quoiqu’ils ne me connaissaient pas encore. Cette équipe, quoique jeune, est très dynamique et chaleureuse, c’est une équipe motivante qui stimule la connaissance. J’ai dû apprendre beaucoup de choses par rapport à eux.
	Finalement, je présente mes sincères remerciements à toute la communauté universitaire, à DIRO en particulier. Merci pour la formation prestigieuse que vous m’avez fournie. Cette formation est si solide qu’elle m’aidera rapidement à intégrer le marché du travail sans perdre de temps. 
	
\newpage

\chapter*{Préface}
Nous développons ce rapport en vue d'achever notre étude de maîtrise en informatique faite à l'université de Montréal. Nous avons inscrit notre maîtrise avec option stage, ce qui exige la redaction d'un rapport au terme de ces études. Nous avons, d'abord réalisé le stage à l'entreprise ML+ située à l'adresse suivante: 338, rue St-Antoine est, Montréal. C'est une jeune entreprise dynamique et spécialisée en science de données. Elle développe des outils pour aider les entreprises à prendre leurs décisions stratégiques tout se basant sur des données historiques des entreprises. \\

C'est une expérience enrichissante d'avoir eu l'opportunité de faire ce stage dans cette entreprise, j'ai pu apprendre beaucoup de choses tant sur machine learning que sur les sciences de données en général. Ces connaissances acquises m'ont ouvert  la porte pour pouvoir rédiger ce rapport. Ce dernier permet d'évaluer le risque d'un patient de retourner chez lui après une opération contre l'arthrose (douleur ressentie au niveau des os).  

\newpage

\listoffigures

\newpage

\listoftables

\newpage

\addcontentsline{toc}{chapter}{Résumé}
\chapter*{Résumé}
L'arthroplastie totale de la hanche et du genou permet à diminuer considérablement la douleur
et améliore la fonction chez les personnes atteintes d'arthrose avancée. Selon une étude publiée en 2012 dans le journal ISRN Orthopedics, les auteurs Bronislava Bashinskaya et al. \cite{key2} ont constaté que : \guillemotleft Le
vieillissement de la génération du "baby boom", combiné au désir de
maintenir un mode de vie actif et sans douleur, entraînera une augmentation
du nombre annuel de chirurgies de remplacement articulaire pratiquées dans
États Unis\guillemotright. Dans un rapport publié en Avril 2007 par les orthopédistes Steven M Kurtz, Kevin Ong, Edmund Lau et Michael T Halpern \cite{key3}, membres de l'Académie américaine des chirurgiens orthopédiques, ils ont prédit que : \guillemotleft  
D'ici 2030, la demande d'arthroplasties totales de la hanche totale devrait augmenter de 174\% pour atteindre 572 000. La demande pour les arthroplasties totales du genou primaire devrait augmenter de 673\% à 3,48 millions de procédures
 \guillemotright.
Face à l'augmentation continue du nombre de chirurgies de remplacement, il devient crucial aux administrateurs des hôpitaux de déterminer si les patients doivent rester à l'hôpital pour une réadaptation après leur 	chirurgie ou s'ils doivent être renvoyés à la maison puisque les coûts associés à une réadaptation doivent être pris en charge par les hôpitaux. Il est important de souligner que la majorité des compagnies d'assurance santé aux États Unis n'assument pas les coûts liés à la réadaptation des patients. Ces frais qui sont très élevés (entre 15,000 à 30,000 \$ par patient) doivent être assurés par les hôpitaux. Pour prendre la décision de retour ou non, les chirurgiens tiennent compte des antécédents médicaux du patient et l'état de santé du patient après l'arthroplastie, ce qui est un travail difficile à effectuer manuellement puisque le nombre de patients dans une base de données orthopédiques sont très nombreux. Dans notre projet de stage, nous avons développé un modèle de machine learning (ML) qui aide à prendre cette décision de façon automatique. Nous avons aussi construit un web API qui fait la connexion avec le modèle, ce qui est utile pour les orthopédistes qui n'ont pas de connaissances en ML. On a conçu un interface utilisateur qui rend notre système convivial et facile à utiliser. 
   	
\newpage
\chapter{Introduction générale}	
\section{Contexte}
Le système de santé américain est contrôlé en majeure partie par des compagnies privées, le gouvernement américain ne finance qu'une partie minoritaire de la population. Ce système  repose sur deux sources de financement. D'abord on a le financement public géré par le programme fédéral Medicare qui vise seulement les personnes âgées de plus de 65 ans et celles qui sont gravement handicapées. D'après Europusa \cite{key4}, une compagnie européenne spécialisée dans l'accompagnement des européens qui veulent immigrer aux états-unis : \guillemotleft Cette source de financement couvre 26\% de la population américaine dont 15 \% sont des personnes agées et handicapées et 11\% sont des familles pauvres avec enfants.  \guillemotright. La deuxième source de financement qui est un financement privé touche tout le reste de la population soit 74\%. L’assurance est donc majoritairement privée aux Etats-Unis. Les Américains sont assurés en général via leurs employeurs ou sinon de manière individuelle lorsque leur employeur ne propose pas d’assurance ou qu’ils travaillent comme travailleurs autonomes. La composante « 	assurance médicale » dans le choix d’un emploi est donc un critère important. En général le système de santé américain est échoué parce qu'une grande partie de la population se retrouve sans assurance santé. C'est un système libéral fondé sur le principe du marché et de la concurrence qui s’organise autour d’assurances privées souvent liées à l’emploi et d’une assurance maladie obligatoire. \\

Les compagnies d'assurance santé américaines ne couvrent pas les coûts associés à une réadaptation d'un patient après une arthroplastie (Opération ayant pour but de rétablir la forme et la mobilité d'une articulation abîmée ou bloquée.). Il revient à l'hôpital de couvrir ces coûts qui sont très élevés. Selon le docteur orthopédiste Jonah Hebert-Davies \footnote{Pour informations sur le docteur, consultez http://www.orthop.washington.edu/?q=faculty-profiles/jonah-hebert-davies-md-frcsc.html} qui a fait ses études de spécialité en Orthopédie à l'université de Seattle, Washington aux États-Unis, les coûts estimés sont entre 15,000 \$ et 30,000 \$ par patient. Ce qui revient à une grande perte financière pour les hôpitaux.  La décision de maintenir ou de renvoyer un patient après une arthroplastie devient une question préoccupante dans la mesure où elle pourrait réduire le coût budgétaire des hôpitaux. Si la décision prise est de renvoyer le patient, l'hôpital est le gagnant sinon il est le perdant. \\ 

D'autres part, l'arthrose (maladie qui touche les articulations et caractérisée par la douleur et la difficulté à effectuer des mouvements articulaires \cite{key12}) touche environ 27 millions d'adultes aux États-Unis \cite{key11}.  Ce qui augmente considérablement le nombre d'arthroplastie dans les hôpitaux, et aussi le poids de travail des médecins à savoir quand est-ce qu'il ya une réadaptation ou non.\\

Il est un travail fastidieux pour les spécialistes orthopédistes de vérifier manuellement les données médicales d'un patient pour pouvoir décider s'il doit être renvoyé ou non. Ce travail consiste en ce que les données médicales d'un dossier patient sont très volumineuses. Seulement pour déterminer les conditions médicales antécédentes du patient, quarante-six colonnes pré-opératoires (conditions de santé du patient avant l'opération) sont prises en compte. Plus tard nous verrons tous les calculs qu'il faut faire avant de prendre cette décision. Dans notre travail nous avons créé des processus automatisés pour faire toutes ces vérifications.  \\

Considérant le nombre grandissant d'arthroplastie dans les hôpitaux américains et tous les travaux manuels qu'un orthopédiste doit réaliser avant de décider le retour ou non d'un patient à la maison, nous estimons que créer un outil pour automatiser cette tâche est un projet pertinent. Le but de notre stage consiste à développer cet outil qui aidera les orthopédistes américains à prendre cette décision de façon automatique et qui les évite à dépenser tout leur temps à analyser un dossier patient parmi plusieurs milliers de dossiers. Dans un premier temps nous avons construit un modèle ML et ensuite nous avons conçu un web API pour faire la connexion avec le modèle. Un interface utilisateur web a été implémenté pour rendre que notre système soit convivial à utiliser.\\

\section{Ingénierie des besoins}

Nous supposons qu'il existe déjà un jeu de données disponible contenant toutes les informations touchant un grand nombre de patients et que chaque échantillon de ce jeu de données est un dossier patient correct faisant l'objet d'une décision de retour. Nous supposons aussi que ce jeu de données possède des caractéristiques suffisantes pour décider si un patient doit être renvoyé ou non. Nous considérons aussi que ce jeu de données est constitué de patients qui ont subi une décision de retour dans le passé. Si toutes ces conditions sont réunies, nous pouvons profiter des différentes technologies de la science de données, du marching learning et l'ingénierie de logiciels pour développer un outil d'aide à la décision qui aide les orthopédistes à prendre la décision de retour automatiquement. \\

Les technologies de la science de données sont conçues pour développer des outils qui permettent de prendre des décisions à partir d'un ensemble de données existantes. Le machine learning est un sous-ensemble de la science de données qui permet de modéliser à partir d'un ensemble de données. Dans le chapitre \ref{chap:background}, nous passons en revue le cycle de travail complet du scientifique de données. Ce que nous proposons comme solution dans notre projet. Nous avons suivi ce cycle complet pour implémenter notre système d'aide à la décision. \\

Il y a deux grandes phases impliquées dans le processus de développement de notre projet: la première phase est la construction d'un modèle machine learning et la deuxième est le déploiement du modèle construit. Pour construire le modèle nous avons utilisé plusieurs techniques de machine learning telles que le re-échantillonnage et l'optimisation.  La deuxième phase consiste en la création d'un API nommé Ortho, c'est surtout une phase d'ingénierie de logiciels. Afin de modéliser les deux phases du développement de notre système et permettre de visualiser sa conception à très haut d'abstraction nous avons crée deux diagrammes UML (Unified Modeling Language). Le premier est un diagramme de cas d'utilisation qui donne une vision globale du comportement fonctionnel de notre système et le deuxième est un diagramme d'états qui explique le processus du développement du modèle. Nous donnons une brève explication de chaque diagramme et leurs différents composants. Pour plus de commodité dans nos schémas, nous choisissons d'écrire les textes en anglais car les textes français sont souvent beaucoup plus longs pour exprimer les idées. Mais dans toutes les explications nous prenons le soin de donner une traduction française pour faciliter la compréhension.\\

\subsection{Diagramme de cas d'utilisation}
Un cas d'utilisation est une situation où un système est utilisé pour décrire les besoins des utilisateurs. On spécifie seulement ce que le système est supposé de faire c'est-à-dire les besoins fonctionnels du système. A ce stade on évite de spécifier ce que le système ne doit pas faire soit les besoins non-fonctionnels du système. Le diagramme des cas d'utilisation est un bon point de départ à propos de toutes les facettes du développement, de la conception, des tests et de la documentation des systèmes orientés objet. Parce qu'il décrit strictement les besoins du système d'un point de vue extérieur; il spécifie les valeurs que le système offre aux utilisateurs \cite[~chapter 2]{key48}. 
Notre diagramme de cas d'utilisation décrit la phase de déploiement de notre modèle ML. Ce diagramme montre les différentes fonctionnalités notre système que les orthopédistes américains auront accès. la figure \ref{usecase} donne une vue schématique de notre diagramme; chaque cas représente une unité discrète d'interaction entre un utilisateur (Orthopédiste ou administrateur) et notre système. Voici une brève explication des acteurs et de chaque cas:
\begin{enumerate}
	\item \textbf{Actors.} Les acteurs sont des orthopédistes et les administrateurs du système qui sont des orthopédistes ayant des droits et privilèges d'évaluer, de suivre et de gérer le modèle déployé;
	\item \textbf{Predict.} Les orthopédistes peuvent utiliser cette section en vue de prédire si les patients doivent rester à l'hôpital ou retourner chez eux. Pour accomplir cette tâche, il est obligatoire de remplir un formulaire.
	\item \textbf{Fill form.} Pour prédire, il faut remplir le formulaire qui contient les informations du dossier médical du patient. On recueille les informations pertinentes pour la prise de décision telles que les conditions médicales, les diagnostiques antérieures, l'indice de masse corporelle (BMI en anglais) etc. 
	\item \textbf{Evaluate.} Permet d'évaluer le modèle sur de nouvelles données prédites. On peut comparer l'évaluation faite au moment de la construction du modèle et celle faite au moment de la prédiction. L'évaluation exige une authentification.  
	\item \textbf{Monitor.} Permet de suivre périodiquement le modèle pour déterminer si les performances s'améliorent ou si elles détériorent. Exige une authentification.
	\item \textbf{Manage.} Permet d'optimiser le modèle sur de nouvelles données en essayant d'utiliser de nouveaux hyper-paramètres en vue de comparer les résultats. Exige une authentification.
	\item \textbf{Authenticate.} Permet l'authentification du système. Exige un nom utilisateur et un mot de passe.
\end{enumerate}

\begin{figure}[h]
	\includegraphics[width=17cm, height =9cm]{images/UseCaseDiag.png}
	\caption{Diagramme cas d'utilisation de notre système}
	\label{usecase}
\end{figure}  

\subsection{Diagramme d'états}
Un diagramme d'états est un diagramme de comportement qui montre le comportement discret d'une partie d'un système conçu par des transitions à états finis. Une transition, dessinée comme une flèche qui connecte les états, représente un changement d'état ou comment passer d'un état source à un état cible. Un état est une condition d'être à un certain moment, il peut être actif ou inactif. Il est actif quand une transition est entrée et il devient inactif quand une transition sort \cite[~chapter 14]{key48}. Dans notre diagramme d'états (voir figure figure \ref{sdml}) nous avons défini plusieurs états et transitions lesquels décrivent les différents comportements de notre système machine learning. Voici un bref résumé de ce que fait chaque état de ce diagramme: 

\begin{enumerate}
\item \textbf{Initial state.} C'est le point de départ du système. Il annonce la construction d'un modèle ML. 
\item \textbf{Getting historical data.} Permet de récupérer les données à partir d'un fichier \textit{csv}. Les données historiques sont un jeu de données disponible avec des valeurs correctes pour entrainer les modèles;
\item \textbf{Explore raw data.} Permet d'explorer les données afin de prendre connaissance de l'ensemble de données. A cet état nous pouvons entre autres voir les données impertinentes, différentes corrélations qui existent entre les variables et l'étiquette;
\item \textbf{Cleaning data.} Dans cet état nous nettoyons nos données en imputant les données nulles et supprimant les variables que ne sont pas pertinentes;
\item \textbf{Transformation data.} Création de nouvelles variables en fonction de celles qui existent déjà dans l'ensemble des données. 
\item \textbf{Modeling data.} C'est le fait d'appliquer les algorithmes de machine learning sur un ensemble de données bien préparé. C'est un état composé de quatre états: 
\begin{enumerate}
	\item \textbf{Training data.} C'est la phase d'entrainement de modèles pour déterminer lequel est meilleur pour notre ensemble de données;
	\item \textbf{Choosing model.} Sélectionner le meilleur modèle après une évaluation de chaque modèle entrainé;
	\item \textbf{Prediction.} Utilisation du meilleur modèle sur un ensemble de données de test;
	\item \textbf{Serialize.} Sauvegarde le meilleur modèle pour son utilisation future.
\end{enumerate} 
\item \textbf{État final.} Le modèle construit est sérialisé pour le système de déploiement. 
\end{enumerate}


\begin{figure}[h]
	\includegraphics[width=11.53cm, height =10.42cm]{images/sdml.png}
	\caption{Diagramme d'états de notre système ML}
	\label{sdml}
\end{figure} 

La suite de ce rapport est structuré de la façon suivante : Le chapitre \ref{chap:background} donne un contexte théorique des différents concepts de la science de données et du machine learning qui sont importants pour notre projet. Le chapitre \ref{chap:modelml} décrit en détail la construction de notre modèle machine learning. Le chapitre \ref{chap:ortho} décrit la création de notre API \textit{Ortho} qui fait la connexion avec le modèle machine learning. Le chapitre \ref{chap:evaluation} montre comment nous évaluons les différents morceaux de notre système. Le chapitre \ref{chap:relatedworks} détaille les travaux connexes liés au développement d'API machine learning, la technique de re-échantillonnage, aux algorithmes ensemblistes etc. Le chapitre \ref{chap:conclusion} fait la conclusion générale de notre rapport.


\newpage

\chapter{Contexte théorique} \label{chap:background}
\section{Introduction}

Tout projet informatique suppose qu'on doit délivrer un produit fini et utilisable accomplissant un certain nombre d'objectifs. Il y a toujours une équipe qui travaille durement pour réaliser ou implémenter ces genres de projets. Tout développement informatique passe nécessairement par la pratique ce qui exige souvent l'application de beaucoup de concepts étudiés en théorie. Dans tout rapport portant sur la réalisation d'un projet scientifique et technique, il s'avère utile et important d'établir un contexte théorique dans le but de faire ressortir les différents cadres théoriques. Nous allons, dans ce chapitre, présenter quelques concepts théoriques importants abordés dans le cadre de notre projet.\\

Généralement, nous nous mettons l'accent sur un sous-ensemble de tous les concepts jugés pertinents pour la réalisation de notre projet. Par exemple, le machine learning est un champ de recherche informatique de plusieurs décennies de travaux de recherche, il est impossible de d'aborder dans un seul chapitre tout ce qui a trait au ML, par contre nous nous concentrons sur les éléments les plus essentiels pour notre projet. \\      

La suite de ce chapitre se subdivise comme suit : la deuxième section est une approche globale sur les sciences de données. Nous estimons que c'est important parce qu'aucun modèle de machine learning ne peut être construit sans d'abord réaliser une phase d'exploration de données; or les sciences de données peuvent être définies comme étant une combinaison entre l'exploration de données et la modélisation de données. La troisième section se porte sur le machine learning qui est le cœur même de notre projet; comme on avait précisé au paragraphe antérieur, nous considérons seulement un sous-ensemble de concepts jugés utiles pour notre projet. La quatrième traite du problème de classification en ML; puisque notre projet était un problème de classification, nous décidons de le consacrer une section indépendante. La cinquième et dernière se porte sur les classes déséquilibrées car notre ensemble de données était déséquilibrée.     

\section{Sciences des données} \label{datascience}

On ne peut pas parler de machine learning sans avoir parlé des sciences de données. Car la modélisation des données est une étape des sciences de données, quoique la plus importante. Les sciences de données nous permettent de prendre un ensemble de données brutes, les explorer et les modéliser dans le but d'extraire des connaissances utiles à la prise d'une décision. Comme son nom l'indique, la science des données n'est possible que lorsqu'on dispose d'un jeu de données qu'on appelle données historiques. Ces dernières sont des données recueillies à partir de la collecte d'informations. Les données peuvent être sous diverses formes, structurées ou non. D'une façon générale, pour traiter un problème en sciences de données, trois conditions sont nécessaires : (1) il faut avoir les données disponibles dans un format adéquat; (2) définir des objectifs précis de ce qu'on veut faire avec les données; (3) et savoir comment est-ce qu'on va procéder pour réaliser ces objectifs. \\

Les sciences de données peuvent être considérées comme une jonction entre l'informatique et les statistiques. Du coté de l'informatique, c'est la modélisation de données et tandis que du coté des statistiques, c'est l'exploration des données. Nous définissons la modélisation comme étant l'utilisation d'un algorithme d'apprentissage sur un ensemble de données bien préparé; on entend par \textit{bien préparé}, quand un jeu de données ne contient pas des données nulles, vides et textuelles.  Quant à l'exploration,  c'est la phase de visualisation et de préparation des données brutes qui peuvent être sous diverses formes. \\

Les sciences de données offrent des méthodes automatisées pour l'analyse prédictive d'un ensemble de données généralement massif. Le but est d'apporter des solutions à des problèmes spécifiques. Au début de l'ère de de l'intelligence artificielle, l'analyse des données massives était compliquée parce que la capacité de calcul des ordinateurs était faible. Avec l'augmentation exponentielle des capacités de ordinateurs, l'explosion quantitative des données numériques devient de moins en moins un problème. Il faut noter qu'en Machine Learning, plus on a de données, plus l'analyse prédictive est fiable. Les algorithmes d'apprentissage fonctionnent mieux avec de grandes quantités de données. Quand le jeu de données est volumineux, on peut faire facilement la validation croisée qui est une technique par laquelle on subdivise aléatoirement un ensemble de données en plusieurs parties dans le but d'obtenir un meilleur apprentissage. La grande quantité de données permet aussi un séparation de données adéquate entre l'ensemble d'entrainement et l'ensemble de test.\\  

Grâce à la croissance de la capacité de stockage des ordinateurs, l'apparition des algorithmes d'apprentissage puissants et la puissance de calcul des ordinateurs, la profession de \textit{scientifique de données} a vu le jour. Yannis Chaouche \footnote{https://openclassrooms.com/courses/initiez-vous-au-machine-learning/comment-resoudre-un-probleme-de-data-science}, professeur et expert en machine learning, a publié sur le site web OpenClassrooms, un cours intitulé  \guillemotleft \textit{Initiez-vous au machine learning}\guillemotright  où il a défini le cycle de travail d'un scientifique de données. D'après lui ce cycle comprend:

\begin{pquotation}{Professeur Yannis Chaouche}
		\noindent 1. la récupération des données utiles à l'étude;\\
		2. le nettoyage des données pour les rendre exploitables;\\
		3. une longue phase d'exploration des données afin de comprendre en profondeur l'articulation des données;\\
		4. la modélisation des données;\\
		5. l'évaluation et interprétation des résultats;\\
		6. la conclusion de l'étude : prise de décision ou déploiement en production du modèle.
\end{pquotation}  

La figure \cite{key6}, implémentée par le professeur Yannis,  nous donne une vue générale du travail d'un scientifique de données. Cette figure est une façon de schématiser les différentes étapes qu'on a définies précédemment. Comme stagiaire en science de données, nous avons suivi minutieusement, dans le cadre de notre projet, le cycle de travail complet défini par le professeur Yannis. Depuis la récupération des données jusqu'au déploiement d'un système en production. Nous expliquerons en détail dans les deux prochains chapitres comment nous implémentons ce cycle au sein de notre projet. 
\begin{figure}[h]
\includegraphics[]{images/workflow_datascientis.png}
\caption{Cycle de travail d'un scientifique de données}
\label{workflow_datascientis}
\end{figure}

\subsection{Données.}

Comme nous avons précédemment dit, les données sont éminentes pour un travail de scientifique de données. S'il n'y a pas de données ou si elles ne sont pas disponibles, on ne peut rien faire. Dans les organisations les données n'ont pas les mêmes niveaux d'importance, par exemple les données juridiques sont de haute confidentialité, personne ne peut avoir accès à moins que les concernés donnent une autorisation légale. Il est toujours difficile d'obtenir les données parce que cela peut compromettre une institution; souvent il faut signer un contrat de confidentialité qui prévoit une sanction sévère s'il y a une violation.  Les données constituent la ressource principale pour qu'un scientifique de données puisse effectuer son travail correctement. 

\subsubsection{Types de données} 
Les données, de part leurs natures, peuvent être quantitatives ou qualitatives. Pour mieux explorer un ensemble de données, il faut savoir distinguer le type de chaque donnée. Quand on parle de type de données ici, il ne faut pas confondre avec type de données qu'on a l'habitude d'utiliser en programmation informatique dans les langages à typage dynamique; mais il faut les considérer d'un point de vue ML. 

\paragraph{Données quantitatives.} Les données quantitatives sont des données qu'on peut mesurer ou qui peuvent prendre une valeur quelconque, avec lesquelles on peut faire des calculs numériques tels que la moyenne, l'addition, la multiplication, la division entre autres. Ces types de données permettent de répondre à la question du type \guillemotleft combien \guillemotright. Par exemple combien de patients y-a-t-il dans la base de données? La réponse pourrait être 3200 qui est une donnée quantitative. Il existe deux catégories de données quantitatives : données quantitatives continues et données quantitatives discrètes. Les premières peuvent n'importe qu'elle valeur dans un ensemble de valeurs, par exemple la température, les PIB,le taux de chômage etc. Les deuxièmes ne prennent qu'un nombre limité de valeur dans un ensemble de valeur, par exemple le nombre d'enfants dans une famille, le nombre de chambres dans un appartement etc. 
    
\paragraph{Données qualitatives.} 
Les données qualitatives décrivent les qualités ou les caractéristiques. On ne peut pas les compter ni faire de calcul avec ces données, on peut les comparer ou éventuellement les trier. Les valeurs qualitatives ne peuvent pas être des nombres, sinon un ensemble de modalités; elles permettent de répondre à la question de la forme \guillemotleft Quel type \guillemotright ou \guillemotleft Quelle catégorie \guillemotright. On distingue aussi deux catégories de données qualitatives : données qualitatives nominales (ou catégorielles) et données qualitatives ordinales. Les premières catégories sont celles dont les modalités ne peuvent ordonnées, par exemple la couleur des yeux, le sexe, la région d'appartenance etc. Les deuxièmes catégories peuvent être ordonnées selon un ordre logique, par exemple les tailles des vêtements, le degré d'accord à un test d'opinion (fortement d'accord, d'accord, pas d'accord ou fortement pas d'accord). \cite[p.~12]{key44}

\subsection{Problématique.}
Tout projet de science de données suppose la résolution d'un problème, ce genres de projets commencent toujours par le constat d'un problème. Il faut qu'on soit en mesure de définir clairement ce qu'on veut résoudre et comment on va le résoudre. Généralement, cela demande qu'on s'asseye avec les responsables des organisations, de faire des entrevues avec eux afin de prendre connaissance des différents besoins et d'établir le pourquoi du projet. S'il n'y a pas une problématique clairement défini, le projet de science de données est impossible. \\

Pour conclure la section sur les sciences de données, la chose la plus importante à retenir c'est que deux conditions doivent être réunies pour qu'un projet de sciences de données soit possible : un jeu de données disponible et une définition claire des problèmes à résoudre. Pour notre projet nous avons disposé d'un ensemble de données orthopédiques et tous les objectifs étaient bien élaborés. \\

Dans la section qui suit nous allons voir une brève introduction sur ce quoi le machine learning, ensuite un survol sur les concepts fondamentaux du ML et finalement nous nous concentrerons sur un sous-ensemble de concepts qui touchent notre projet, principalement le problème de classification. Nous n'aborderons pas dans ce rapport, la théorie sur l'exploration des données \textit{(data mining en anglais)}. Nous supposons que ceux qui lisent ce rapport ont déjà des notions élémentaires sur cette question, sinon nous vous proposons de lire \cite{key46, key47}

\section{Vue d'ensemble sur Machine Learning} \label{section:ml}

Dans la section précédente, nous avons pu y voir plus clair sur le cycle global de travail du scientifique de données. Nous allons maintenant parler du machine learning dans cette section, c'est à dire la modélisation des données. Nous utilisons le machine learning probablement des dizaines de fois par jour sans même le savoir. Chaque fois que nous effectuons une recherche Web sur Google ou Bing, cela fonctionne si bien c'est parce que leur logiciel de machine learning a trouvé comment classer les pages. Lorsque Facebook ou l'application photo d'Apple reconnaît nos amis dans nos images, c'est aussi du machine learning. Chaque fois que nous lisons notre courrier électronique et qu'un filtre anti-spam nous évite d'avoir à parcourir des tonnes de spam, c'est parce que nos ordinateurs ont appris à distinguer le spam du courrier non-spam. Donc, c'est du machine learning. En 1997, Tom Mitchell, dans son livre \guillemotleft Machine Learning \guillemotright a proposé une définition plus précise : 

\begin{pquotation} {Tom Mitchell \cite[p.~2]{key37}}
	\guillemotleft On dit qu'un programme informatique apprend de l'expérience \textbf{E} par rapport à une certaine classe de tâches \textbf{T} et à la mesure de performance \textbf{P}, si sa performance aux tâches dans \textbf{T} , telle que mesurée par \textbf{P}, s'améliore avec l'expérience \textbf{E}\guillemotright.
\end{pquotation} 

Nous voyons dans cette définition que le programme informatique peut être n'importe quel algorithme d'apprentissage; l'expérience est considérée comme n'importe quelle source de connaissances auxquelles un algorithme est obligé d'apprendre, par exemple un même algorithme peut apprendre à partir des données de finance, biologie, médecine etc.; La mesure de performance consiste à améliorer les résultats d'un algorithme tout en conservant la même expérience. \\ 

La figure \ref{machinelearning} nous montre en grosso modo les techniques du machine learning, à un très haut d'abstraction. Deux grandes techniques de machine learning sont définies : Apprentissage supervisé \textbf{(Supervised learning en anglais)} et l'apprentissage non-supervisé \textbf{(Unsupervised learning en anglais)}. L'apprentissage supervisé est une technique d'apprentissage avec des données étiquetées, il existe deux types de problèmes possibles dans l'apprentissage supervisé: problème de régression qui consiste à prédire une valeur numérique comme le prix d'un appartement dans une région géographique quelconque; et le problème de classification qui consiste à prédire une valeur booléenne (oui ou non), par exemple on peut vouloir savoir si une campagne marketing pourrait être une réussite ou un échec. L'apprentissage non-supervisé est une technique d'apprentissage non étiquetée, c'est l'idée de groupe \textbf{(Clustering en anglais)} qui prédomine.  Cette figure donne une vue générale sur ce qu'est le machine learning.\\

Le machine learning constitue une manière de modéliser des phénomènes, dans le but de prendre des décisions stratégiques. Les algorithmes utilisés permettent, dans une certaine mesure, à un système piloté par ordinateur (un robot éventuellement), ou assisté par ordinateur, d'adapter ses analyses et ses comportements en réponse, en se fondant sur l'analyse de données empiriques provenant d'une base de données ou de capteurs. La difficulté réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire \textbf{(on parle d'explosion combinatoire)}. On confie donc à des programmes le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que la nature des données d'entrainement n'est pas connue \cite{key13}.

\subsection{Phases de modélisation d'un problème ML}

Nous avons mentionné dans la section \ref{datascience} portant sur les Sciences de données, qu'aucun projet de sciences de données n'est possible s'il n'y pas une problématique bien définie. Et s'il existe un problème, il existe aussi des moyens de le résoudre. Le cycle de travail d'un scientifique de données énuméré dans la section \ref{datascience} peut être comme les principales techniques de résolution d'un problème. Dans ce cycle de travail, il y a la technique de modélisation, voyons maintenant le deux phases nécessaires à la modélisation:
\begin{enumerate}
	\item \textbf{Phase d'entrainement.} Phase dans laquelle on dispose d'un ensemble de données d'entrainement \textbf{(trainset en anglais)} constitué de variables prédictives et une variable cible \footnote{Les variables cibles peuvent aussi appeler \textit{étiquettes}}; les données d'entrainement proviennent des expériences antérieures. Durant cette étape l'algorithme d'apprentissage ne fait que d'apprendre à partir des exemples des données d'entrainement, il ne retourne aucune valeur. C'est pendant cette phase que la fonction d'hypothèse $h$ (ou fonction de prédiction) est sélectionnée dans l'espace des hypothèses $\mathcal{H}$. 
	\item \textbf{Phase de prédiction.} Dans la prédiction, on dispose d'un nouvel ensemble de données où les valeurs cibles ne sont pas connues et ce sont ces valeurs qu'on doit prédire. Dans ce cas la fonction de prédiction est utilisée pour calculer toutes les valeurs possibles de la variable cible et son entrée est l'ensemble des variables prédictives.
\end{enumerate}

Dans la pratique, obtenir de nouvelles données est une tâche très difficile,  peut prendre plusieurs mois d'attente. Par conséquent quand on a un ensemble de données, il est recommandé au scientifique de données, au moment où il construit son modèle, de le répartir en deux : données d'entrainement et données de test. L'ensemble des données d'entrainement est utilisé dans la phase d'entrainement et celui des données de test dans la phase de prédiction.  Habituellement on conserve 80\% des données pour l'entrainement et 20\% pour le test. 

\begin{figure}[h]
\includegraphics[width=15cm, height=6cm]{images/machinelearning.png}
\caption{Vue globale de Machine learning \cite{key24}}
\label{machinelearning}
\end{figure}

Pour continuer nous allons passer en revue les différents types de techniques de machine learning apparues dans la figure \ref{machinelearning}. Elles constituent la base du machine learning. Aussi nous verrons quelques concepts importants découlés de chaque type. Nous donnerons une brève introduction de chaque thème jugé important pour la suite de ce rapport. L'idée n'est pas de tout détailler mais de présenter un aspect global de chaque concept pour faciliter la compréhension.  

\subsection{Apprentissage supervisé}

L'apprentissage supervisé est formellement défini comme une fonction $h:x \rightarrow y $ où $h \in \mathcal{H} $ est la fonction d'hypothèse produite par un apprenant qui est un algorithme d'apprentissage, et $\mathcal{H}$ est l'espace des hypothèses d'apprentissage; $x$ représente l'ensemble de données d'entrée de la fonction et $y$ détermine l'ensemble de données de sortie. La fonction d'hypothèse peut être aussi appelée fonction de prédiction, elle est sélectionnée de telle sorte qu'elle soit très rapprochée de l'ensemble de données.  Une autre définition possible de l'apprentissage supervisé est un ensemble de couples d'échantillons d'entrée-sortie de la forme $(x^{(i)}, y^{(i)})$ où $x^{(i)}$ est la description d'un objet dans un espace de représentation; $y^{(i)}$ est la variable cible désirée de l'objet; et $i$ est un index  défini sur l'intervalle  $1 \leq i \leq m$, où $m$ le nombre d'échantillons ou d'observations dans l'ensemble de données. Le nombre de  variables prédictives associées à une observation est noté comme un vecteur $\vec{x}=(x_1, \cdots, x_j)$ où $j$ est un index compris entre $1 \leq i \leq n$ et $n$ représente la taille des variables \cite{key38, key40, key39}. Les $n$ variables décrivant les $m$ individus de l'ensemble de données sont représentés sous la forme d'une matrice nommée $X$ de dimensions $(m,n)$ qui est définie de la manière suivante :   

$$ 
	X=
		\begin{pmatrix}
			x_{11} & \cdots & x_{1n}\\
			\vdots & \ddots & \vdots \\
			x_{m1} & \cdots & x_{mn}
		\end{pmatrix}
$$      

\begin{flushright}
	\cite[p.~21]{key44}
\end{flushright}                                                               

et un vecteur $\vec{y}$ défini de la manière suivante:
$$
\vec{y}=
\begin{pmatrix}
y_1\\
\vdots\\
y_m
\end{pmatrix}
$$ 

Dans le but de faciliter la compréhension nous estimons qu'il est important de définir certains concepts utilisés dans cette définition :
\begin{itemize}
	\item \textbf{Objet.} Un objet est une instance d'un ensemble de données, on le nomme aussi individu, observation, échantillon ou ligne.
	\item \textbf{Espace de représentation.} L'espace de représentation représente l'ensemble de données.
	\item \textbf{Variables prédictives.} L'ensemble de variables à partir desquelles on peut faire des prédictions.
	\item \textbf{Variable cible.} C'est une variable  qu'on souhaite prédire.
\end{itemize}

Pour expliquer en clair cette définition, laissons-nous l'illustrer à travers un exemple en prenant un échantillon de l'ensemble de données \guillemotleft \textit{California Housing Prices} \footnote{Cet ensemble de données est disponible sur ce site : https://www.kaggle.com/camnugent/california-housing-prices} \guillemotright. Cet ensemble de données a été implémenté par R. Kelley Pace et Ronald Barry \cite{key43}, il contient des informations sur les prix de plusieurs milliers de logements dans toutes les villes de Californie aux États-Unis. Il a 10 colonnes et 20640 lignes de données, nous l'avons utilisé dans notre stage pour mettre en pratique les différents concepts appris de ML.  Pour expliquer la définition, nous choisissons seulement 5 instances de cet ensemble de données, 3 variables prédictives ( \textit{longitude, latitude et median\_income}) et la variable cible qui est \textit{median\_house\_value}. Le problème d'apprentissage est de prédire le prix moyen d'un logement dans une ville de Californie. L'ensemble de données de la table \ref{tab:CaliforniaHousingPricesTrain} est considéré comme un ensemble d'entrainement car toutes les valeurs de la variable cible sont connues. Tandis que les données la table \ref{tab:CaliforniaHousingPricesTest} est l'ensemble de test puisque les valeurs de  \textit{median\_house\_value} sont inconnues.
   

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|c|c|c|}
		\hline
		\textbf{longitude} & \textbf{latitude} & \textbf{median\_income} & \textbf{median\_house\_value}   \\
		\hline
		 	-122.23 & 37.88  & 8.3252	& 452600.0 \\
			-122.22 & 37.86 & 8.3014 & 358500.0 \\
			-122.24 & 37.85 & 7.2574 & 352100.0\\
			-122.25 & 37.85 & 5.6431 & 341300.0\\
			-122.25	& 37.84 & 3.8462 & 342200.0\\
		\hline
		
	\end{tabular}
	\caption{Échantillon de l'ensemble de données \textit{California Housing Prices}}
	\label{tab:CaliforniaHousingPricesTrain}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|c|c|c|}
		\hline
		\textbf{longitude} & \textbf{latitude} & \textbf{median\_income} & \textbf{median\_house\_value}   \\
		\hline
		-122.24 & 37.85  & 6.005	& ? \\
		-122.25 & 37.86 & 5.0519 & ? \\
		\hline
		
	\end{tabular}
	\caption{Échantillon de l'ensemble de données \textit{California Housing Prices}}
	\label{tab:CaliforniaHousingPricesTest}
\end{table}

En suivant la définition de l'apprentissage supervisé et en considérant les données de la table \ref{tab:CaliforniaHousingPricesTrain}, la matrice de données peut être écrite de la façon suivante :
$$ X=
	\begin{pmatrix}
		-122.23x_1 & 37.88x_2 & 8.3252x_3\\
		-122.22x_1 & 37.86x_2 & 8.3014x_3 \\
		-122.24x_1 & 37.85x_2 & 7.2574x_3 \\
		-122.25x_1 & 37.85x_2 & 5.6434x_3 \\
		-122.25x_1 & 37.84x_2 & 3.8462x_3 \\
	\end{pmatrix}	
$$

et le vecteur $ \vec{y}$ de la façon suivante :

$$
	\vec{y}=
	\begin{bmatrix}
		452600 \\
		358500 \\
		352100 \\
		341300 \\
		342200 \\
	\end{bmatrix}
$$
\\
où $x_1$ représente \textit{longitude}, $x_2$ \textit{latitude}, $x_3$\textit{ median\_income} et $y$ median\_house\_value; $m=5$ est le nombre d'observations (ou lignes) de l'ensemble de données; $n=4$ le nombre de variables prédictives. \\
On sait que, d'après la définition : $h(x) = y$, par conséquent on a : 

$$ 
	\begin{pmatrix}
	-122.23x_1 & 37.88x_2 & 8.3252x_3\\
	-122.22x_1 & 37.86x_2 & 8.3014x_3 \\
	-122.24x_1 & 37.85x_2 & 7.2574x_3 \\
	-122.25x_1 & 37.85x_2 & 5.6434x_3 \\
	-122.25x_1 & 37.84x_2 & 3.8462x_3 \\
	\end{pmatrix}	
	=
	\begin{bmatrix}
	452600 \\
	358500 \\
	352100 \\
	341300 \\
	342200 \\
	\end{bmatrix}
$$
Ce qui peut être écrit sous forme d'équations linéaires de la façon suivante : 

\begin{align*}
	-122.23x_1 + 37.88x_2 + 8.3252x_3 = 452600 \\
	-122.22x_1 + 37.86x_2 + 8.3014x_3 = 358500 \\
	-122.24x_1 + 37.85x_2 + 7.2574x_3 = 352100 \\
	-122.25x_1 + 37.85x_2 + 5.6434x_3  = 341300 \\
	-122.25x_1 + 37.84x_2 + 3.8462x_3  = 342200
\end{align*}

La plupart des algorithmes supervisés sont linéaires c'est-à-dire leurs fonctions d'hypothèse $h$ sont une combinaison linéaire des variables prédictives et elle est écrite de la forme: $\theta_1x_1 + \cdots + \theta_nx_n \equiv \theta \cdot X$. Le vecteur $\vec{\theta} $ est défini de la façon suivante : $\vec{\theta} = \theta_1, \cdots, \theta_n$ qui sont les différentes valeurs possibles de chaque variable prédictive. \\
La régression logistique et la classification logistique sont des exemple d'algorithmes linéaires.    

%Les données de la table \ref{tab:CaliforniaHousingPricesTrain} peuvent utilisées pendant la phase d'entrainement puisque les valeurs de la variable sont toutes connues. Tandis que les données de la table \ref{tab:CaliforniaHousingPricesTest} doivent utilisés pendant la phase de prédiction car les valeurs cibles sont inconnues. \\

\subsection{Type d'apprentissage supervisé}

L'apprentissage supervisé, selon le type de sorties, est divisé en deux types de problèmes : problème de classification et problème de régression. La différence entre les deux se situe au niveau du type de problème à traiter. Dans la vie réelle, les problèmes sont diverses et différents les uns des autres mais une analyse prédictive aide à catégoriser les types de problèmes à traiter et déterminer quels types d'algorithmes supervisés à utiliser.  Dans les deux cadres de figure les entrées restent toujours les mêmes, il n'y a aucune distinction à ce niveau. Voyons un peu en détails les caractéristiques de chaque problème et pour pouvoir saisir la différence qui existe entre les deux.  

\subsubsection{Problème de classification} 
Dans un problème de classification, les types de sorties qu'on traite sont des classes. Les classes sont des variables catégorielles ou discrètes, elles permettent de déterminer si un vecteur $x$ appartient à une catégorie de sortie, dans ce cas la variable cible est qualitative. En d'autres termes, le problème de classification est le problème d'identifier lequel d'un ensemble de catégories (sous-population) appartient à une nouvelle observation, sur la base d'un ensemble de données contenant des observations (ou instances) dont la composition est connue. Une définition plus précise et formelle a été proposée par Tom Michell :

\begin{pquotation} {Tom Mitchell \cite[p.~88]{key38}}
	\guillemotleft Un exemple est un couple $(\mathbf{x,u})$ où $\mathbf{x} \in \mathcal{X}$ est la description ou la représentation de l'objet et $\mathbf{u} \in \mathcal{U}$ représente la supervision de $\mathbf{x}$. Dans un problème de classification, $\mathbf{u}$ s'appelle la classe de $\mathbf{x}$ et appartient à un ensemble $\mathcal{C} = \{\omega_1, \cdots \omega_C \}$. $\mathcal{C}$ designe le nombre de classes possibles pour un objet\guillemotright.
\end{pquotation}

Dans la pratique, pour parler de classification, l'ensemble $\mathcal{C}$ doit être fini et petit. Si $\mathcal{C} = 2$, on parle de classification binaire qui est le cas spécial de distinguer exactement deux classes (oui/non ou vrai/faux); si $\mathcal{C} > 2$, on parle de classification multiclasse. L'exemple d'iris \footnote{page de wikipedia sur iris : https://en.wikipedia.org/wiki/Iris\_flower\_data\_set} qui est un jeu de données basé sur 50 échantillons d'espèces végétales est un exemple typique de la classification multiclasse. Un exemple de classification binaire est de prédire si un patient d'un certain lieu géographique peut avoir ou non le cancer si l'on dispose d'un ensemble de données médicales des patients de ce lieu. Les algorithmes d'apprentissage qui résolvent des problèmes de classification sont appelés  \guillemotleft classifier \guillemotright. L'algorithme essaye d'attribuer chaque nouvelle entrée à une classe discrète appartenant au nombre $\mathcal{C}$ de classes. \cite{key44, key41, key42}

%Voici quelques exemples de problèmes de classification. Nous avons déjà parlé de la classification du spam par courrier électronique comme exemple de problème de classification. Un autre exemple serait le classement des transactions en ligne. Donc, si vous avez un site Web qui vend des trucs et si vous voulez savoir si une transaction particulière est frauduleuse ou non, si quelqu'un utilise une carte de crédit volée ou a volé le mot de passe de l'utilisateur. Si la transaction est frauduleuse, on retourne 1 comme valeur sinon l'algorithme retourne 0.\\
%0 est aussi appelé la classe négative, et 1 la classe positive, et ils sont parfois aussi désignés par les symboles "-" et "+".\\

\subsubsection{Mesure de performance des algorithmes de classification}
L'évaluation de la performance des méthodes de machine learning est aussi cruciale que l'algorithme lui-même, car il identifie les forces et les faiblesses de chaque algorithme d'apprentissage. Différentes mesures de performance sont utilisées pour évaluer différents algorithmes de machine learning. Pour ce projet, nous nous concentrons sur celles utilisées dans les problèmes de classification parce que notre projet est problème de classification. Les métriques  de classification couramment utilisées pour évaluer la performance des problèmes de classification sont \textit{Log-Loss, Accuracy, AUC(Area under Curve)} etc. Nous pouvons créer nos propres métriques personnalisées selon nos besoins. Le choix des métriques est très important en ML car il permet de détecter les défauts ou les erreurs de nos modèles. \\

%Les metrics que vous choisissez pour évaluer votre modèle d'machine learning sont très importantes. Le choix des paramètres influence la façon dont la performance des algorithmes d'machine learning est mesurée et comparée. Pour ne pas perdre plus de temps, voyons ce que sont quelques metrics que nous avons utilisé dans notre projet.

\subsubsection{Matrice de confusion}
La matrice de confusion \textbf{(Confusion Matrix en anglais)} est utilisée pour trouver l'exactitude et la précision d'un modèle. Elle doit être utilisée pour des problèmes de classification binaire puisqu'il est basé sur des valeurs positives et négatives.La matrice de confusion en soi n'est pas une mesure de performance en tant que telle, mais presque toutes les métriques de performance sont basées sur la matrice de confusion et les nombres qui s'y trouvent. La figure \ref{confusionmatrix} est une représentation de la matrice de confusion. La Actual value (\textbf{valeur actuelle}) représente la vraie valeur de l'étiquette ou la variable cible tandis que la Predicted value (\textbf{valeur prédite}) représente la valeur de prédiction. 
\paragraph*{True Positives (TP) :}
True Positives (\textbf{vrais positifs, en français}) sont les cas où la classe réelle du point de données était 1 (Vrai) et la prédiction est également 1 (Vrai)
\paragraph*{True Negatives (TN):}
True Negatives (\textbf{vrais négatifs, en français}) sont les cas où la classe réelle du point de données était 0 (Faux) et la prédiction est également 0 (Faux)

\begin{figure}[h]
	\includegraphics[width=14cm, height=7cm]{images/confusionmatrix.png}
	\caption{Confusion Matrix representation \cite{key25}}
	\label{confusionmatrix}
\end{figure}

\paragraph*{False Positives (FP):}
False Positives (\textbf{faux positifs, en français}) sont les cas où la classe réelle du point de données était 0 (Faux) et la prédiction est 1 (Vrai). Faux parce que le modèle a prédit incorrectement et positivement parce que la classe prédite était positive. (1)
\paragraph*{False Negatives (FN):}
False Negatives (\textbf{faux négatifs, en français}) sont les cas où la classe réelle du point de données était 1 (Vrai) et la prédiction est 0 (Faux). Faux parce que le modèle a prédit incorrectement et négativement parce que la classe prédite était négative. (0)

Voyons comment  calculer quelques métriques de classification à partir de la matrice de confusion :

\begin{enumerate}
	\item \textbf{Accuracy:}
	L'exactitude \textbf{(Accuracy en anglais)} dans les problèmes de classification est le nombre de prédictions correctes faites par le modèle sur toutes les prédictions faites.
	$\textnormal{Accuracy} = \frac{TP+TN}{TP+ FP + FN + TN} $
	
	\item  \textbf{Precision:} 
	La précision permet de répondre à la question suivante : Quelle proportion d'identifications positives était effectivement correcte ?
	$\textnormal{Precision} = \frac{TP}{TP+ FP} $
	
	\item \textbf{Recall or Sensitivity:}
	Le rappel permet de répondre à la question suivante : Quelle proportion de résultats positifs réels a été identifiée correctement ?
	$\textnormal{Recall} = \frac{TP}{TP+ FN} $
	
	\item \textbf{F1-score:}
	Nous ne voulons pas vraiment avoir à la fois la précision et le rappel dans nos poches chaque fois que nous faisons un modèle pour résoudre un problème de classification. Donc, il est préférable que nous puissions obtenir un seul score qui représente à la fois la précision (P) et le rappel (R)
	
	$ \textnormal{F1score} = \frac{ 2 }{\frac {1}{\textnormal{precision}} +  \frac {1}{\textnormal{recall}}  } = 2 \times \frac{\textnormal{precision} \times \textnormal{recall}} { \textnormal{precision} + \textnormal{recall} } = \frac{TP} {TP + \frac{FN+FP} {2} } $
\end{enumerate}



\subsubsection{Problème de régression.} 
Dans un problème de régression l'espace de sortie est formé par des valeurs continues. C'est le cas où l'espace de représentation $\mathcal{H}$ est un ensemble de fonctions $h$ à valeurs réelles. L'exemple que nous avons pris pour expliquer l'apprentissage supervisé est un problème de régression car les prix des logements sont des valeurs réelles. Établir la relation entre l'augmentation du prix d'un produit et sa demande, évaluer l'impact d'une campagne publicitaire en fonction des frais engagés sont d'autres exemples d'utilisation de la régression. La différence entre les problèmes de classification et de régression est la suivante: Dans la classification, la variable cible est qualitative c'est-à-dire c'est une variable à valeurs discrètes ou catégorielles tandis que dans la régression la variable cible est quantitative c'est une variable à valeurs continues. 

\subsection{Apprentissage non supervisé}
 
 L'apprentissage supervisé est une tache de machine learning qui consiste à inférer une fonction qui décrit la structure des données à variables non-étiquetées c'est-à-dire il n'y a pas de variable cible, toutes les données sont équivalentes (il n'y a que de variables d'entrées, pas de sortie); dans ce cas les algorithmes cherchent à organiser les données en groupes (\textbf{Clusters en anglais}). Chaque groupe doit comprendre des données similaires et les données distincts doivent se retrouver dans des groupes distincts. S'il existe la possibilité de trouver un partitionnement (\textbf{Clustering en anglais}) tel que les similarités des objets dans un groupe sont beaucoup plus grands que les autres groupes, on peut considérer que l'ensemble d'entrainement peut être représenté par un seul groupe structuré.  La similarité entre les objets est généralement calculée selon une fonction de distance entre les couples d'échantillons. La distance communément utilisée est la distance euclidienne définie de la cette manière:
 \begin{align*}
 \text{Soit une matrice } X  \text{ à } n \text{ variables quantitatives } \\
 \text{Soient deux échantillons } (x_1, x_2) \\
 \text{La distance euclidienne est calculée par la formule suivante:}\\
 d(x_1,x_2) = \sqrt{\sum_{j=1}^{n} (x_{1n}-x_{2n})}
 \end{align*} 
 
 Les possibilités d'application de l'apprentissage non-supervisé sont nombreuses, par exemple on peut analyser une base de données marketing pour grouper les clients qui ont des comportements similaires auxquels on pourrait adresser des campagnes commerciales personnalisées. D'autres cas d'application serait le traitement des images, la segmentation des textes, la détection d'une anomalie dans les systèmes de contrôle entre autres.  \cite{key41, key44}\\
 
 Deux approches sont proposées pour partitionner un ensemble de données de façon automatique :
 \begin{enumerate}
 	\item \textbf{Partitionnement hiérarchique.} C'est une approche qui consiste à repartir un ensemble de données dans une suite de groupes emboités les uns dans les autres; dans ce cas les relations entre les groupes sont représentés dans une structure arborescente. Plus on est plus bas dans l'arbre, plus les objets se ressemblent. Cette méthode est applicable lorsqu'on ne connait pas à l'avance le nombre de groupes à former.
 	\item \textbf{Partitionnement non-hiérarchique.} Les instances de l'ensemble des données sont réparties en un nombre $k$ de classes. $k$ étant une valeur fixée à l'avance. 
 \end{enumerate}  
 
 Dans les deux approches une bonne classification (\textbf{clustering en anglais}) est réussie quand les groupes ont une forte similarité intraclasses (c'est-à-dire les objets d'un même groupe doivent se ressembler) et une fiable similarité interclasses (c'est-à-dire les objets d'un groupe distinct ne doivent pas se ressembler). \cite[p.~86]{key44}


   
\subsection{Classes déséquilibrées} \label{subsec:imbalancedclasses}

Nous terminerons la section \ref{section:ml} sur le machine learning en abordant un sujet très important en machine learning, surtout dans un problème de la classification qui est "\textit{Jeux de données déséquilibré}" \textbf{(Imbalanced datasets or Imbalanced Classification or Imbalanced Classes, ses différentes appellations en anglais)}. C'est un problème très commun en machine learning, spécialement en classification. Nous abordons ce sujet parce que c'était l'une des principales difficultés de notre projet. \\

Une distribution de classe déséquilibrée est un scénario où le nombre d'observations appartenant à une classe est significativement inférieur à celui des autres classes. 
Ce problème prédomine dans les cas où la détection d'anomalies est cruciale comme le vol d'électricité, les transactions frauduleuses dans les banques, l'identification de maladies rares, etc. Dans ce cas, le modèle prédictif développé avec des algorithmes conventionnels pourrait être biaisé et inexact.\\
Cela arrive parce que les algorithmes de machine learning sont généralement conçus pour améliorer la précision en réduisant l'erreur. Ainsi, ils ne tiennent pas compte de la répartition / proportion des classes ou de l'équilibre des classes .\\

Jason V. Hulse et al. \cite{key45} ont proposé plusieurs techniques permettant de manipuler les classes déséquilibrées. Voyons un bref résumé de quelques-unes d'entre elles:

\begin{itemize}
\item \textbf{Sous-échantillonnage aléatoire.} Le sous-échantillonnage aléatoire (\textbf{Random Undersampling, en anglais})  vise à équilibrer la distribution des classes en éliminant de manière aléatoire les exemples de classes majoritaires. Ceci est fait jusqu'à ce que les instances de la majorité et de la classe minoritaire soient équilibrées.
\item \textbf{Sur-échantillonnage aléatoire.} Le sur-échantillonnage aléatoire (\textbf{Random Over-Sampling, en anglais}) augmente le nombre d'instances dans la classe minoritaire en les reproduisant aléatoirement afin de présenter une représentation plus élevée de la classe minoritaire dans l'échantillon.
\item \textbf{Suréchantillonnage en groupe (Cluster-Based Over Sampling, en anglais).}
Dans cette technique, l'algorithme de clustering K-means est appliqué indépendamment aux instances de classe minoritaire et majoritaire. Cela permet d'identifier les clusters dans l'ensemble de données. Par la suite, chaque grappe est suréchantillonnée de sorte que tous les clusters de la même classe ont un nombre égal d'instances et toutes les classes ont la même taille.
\item \textbf{Synthetic Minority Over-sampling Technique(SMOTE):}
Cette technique est suivie pour éviter le surapprentissage qui se produit lorsque des répliques exactes d'instances minoritaires sont ajoutées à l'ensemble de données principal. Un sous-ensemble de données est pris à partir de la classe minoritaire à titre d'exemple, puis de nouvelles instances similaires synthétiques sont créées. Ces instances synthétiques sont ensuite ajoutées à l'ensemble de données d'origine. Le nouvel ensemble de données est utilisé comme un échantillon pour former les modèles de classification.
\item \textbf{Modified synthetic minority oversampling technique (MSMOTE):}
C'est une version modifiée de SMOTE. SMOTE ne tient pas compte de la distribution sous-jacente de la classe minoritaire et des bruits latents dans l'ensemble de données. Pour améliorer les performances de SMOTE, une méthode modifiée MSMOTE est utilisée.
\end{itemize}

\subsection{Méthodes d'ensemble} \label{subsect:ensemblemethod}

L'objectif principal de la méthodologie d'ensemble \textbf{(Ensemble methods en anglais)} est d'améliorer la performance des classificateurs uniques. L'idée de base de l'apprentissage d'ensemble est de combiner plusieurs algorithmes simples afin d'obtenir un qui soit plus complexe dans le but de générer de meilleurs résultats. Ces résultats sont beaucoup mieux que lorsqu'on utilise un seul algorithme.  L'approche consiste à construire plusieurs classificateurs à deux étapes à partir d'un ensemble de données original, puis à agréger leurs prédictions \cite{key1, key39}. Les méthodes ensemblistes fonctionnent aussi bien sur de petit ensemble de données que sur de grand volume de données.

Les méthodes d'ensemble sont nombreuses, mais nous allons brièvement définis les trois plus importantes: Bagging, Boosting et Random Forest. 

\paragraph{Bagging}
Bagging (Bootstrap aggregating) est une approche de construction d'ensemble qui 
utilise différents sous-ensembles de données d’apprentissage avec une  méthode  de classification unique.  Étant  donné  un  ensemble  d’apprentissage  de  taille \textbf{t}, Bagging attire des instances aléatoires \textbf{t} de l'ensemble de données avec remplacement (à l'aide d'une distribution uniforme). Ces \textbf{t} instances sont apprises, et ce processus est répété plusieurs fois. Étant donné que le tirage au sort est effectué avec remplacement, les   instances   tirées   contiendront des   doublons   et   des   omissions   par   rapport   à l'ensemble  d'apprentissage initial.  Chaque  cycle  à  travers  le  processus  aboutit  à  un classificateur.  Après  la  construction  de  plusieurs  classificateurs,  les  sorties  de  chaque classificateur sont combinés pour produire la prédiction finale \cite{key27}. 
%Méthodes parallèles (ensachage, forêt aléatoire) dont le principe est de faire la moyenne de plusieurs prédictions dans l'espoir d'un meilleur résultat suite à la réduction de la variance de l'estimateur moyen.
\paragraph{Boosting.}
Une  autre  approche  appelée  «Boosting»  utilise  également une  méthode d'apprentissage  unique  avec  différents  sous-ensembles  de  données  d’apprentissage. Sa structure globale est similaire à celle de la méthode Bagging, à la différence qu’elle  conserve  la  trace  de  la  performance  de  l'algorithme  d'apprentissage  et  se concentre  sur  les  cas  qui  ne  sont  pas  correctement  appris.  Au  lieu  de  choisir  les instances \textbf{t} d’apprentissage à l'aide d'une distribution uniforme de manière aléatoire, les exemples d’apprentissage sont sélectionnés en favorisant les instances quine sont pas  bien  classées.  Après  plusieurs  cycles,  la  prédiction  est  réalisée  selon  un  vote pondéré des prédictions de chaque classificateur. Ainsi, les poids sont proportionnels à la précision de chaque classificateur sur son ensemble d'apprentissage. L’algorithme le plus connu de l’approche Boosting, appelée « AdaBoost » \cite{key27}.
\paragraph{Stacking.}
Le stacking est similaire aux boosting et bagging: le but est de combiner également plusieurs modèles à un ensemble données d'origine. La différence est que le stacking permet d'explorer différents modèles pour le même problème. Il est possible d'entrainner différents types d'apprenants simultanément. Le stacking divise l'ensemble de données en deux sous-ensembles, le premier est utilisé durant la phase d'entrainement et le deuxième est conservé pour le test. Le mécanisme est que la sortie d'un classifier est utilisé comme données d'entrainement pour un autre classifier.  

%cependant, que vous n'avez pas juste une formule empirique pour votre fonction de poids, plutôt que vous introduisez un méta-niveau et utilisez un autre modèle / approche pour estimer l'entrée avec les sorties de chaque modèle pour estimer les poids ou , en d'autres termes, pour déterminer quels modèles fonctionnent bien et ce qui est mal donné ces données d'entrée \cite{key28}. 
\paragraph{Random Forest.}
Les forêts aléatoires(plus connus sous \textbf{Random Forest}) sont une combinaison d'arbres  de  décision,  où  chaque  arbre  dépend  des  valeurs  d'un  vecteur  aléatoire indépendamment échantillonné et avec la même distribution pour tous les arbres de la forêt.L'erreur  de  généralisation  d'une  forêt  d'arbres  dépend  de  la  force  des  arbres individuels  dans  la  forêt  et  de  la  corrélation  entre  eux.  L'utilisation d'une  sélection aléatoire  de  caractéristiques  pour  diviser  chaque nœud donne des  taux  d'erreur  qui se comparent favorablement à AdaBoost. \cite{key1}\\

\section{Conclusion}
Le chapitre 2 a présenté une vue approche théorique de quelques grands concepts de machine learning et de sciences de données, ceux que nous jugeons être importants pour la réalisation de notre projet. Nous avons donné une brève définition de chaque concept tout en essayant d'être le plus clair et précis possible. Nous avons vu un sous-ensemble de sujets importants des différents concepts puisque ces champs d'études sont très vastes et contiennent beaucoup de choses intéressantes, mais nous avons mis accent sur ce qui nous intéresse pour notre projet. Par exemple notre projet est un problème de classification en machine learning, pour cela nous avons dédié une section à une étude brève du problème de classification Tout en ayant le soin de définir et d'expliquer les thèmes les plus pertinents. Aussi notre dataset est déséquilibré, par conséquent nous avons mis l'accent sur c'est quoi les classes déséquilibrées et les différentes techniques permettant de les manipuler. Et en dernier lieu nous avons passé en revue les méthodes ensemblistes puisque les algorithmes d'ensemble sont utilisés pour attaquer le problème de classes déséquilibrées. Le chapitre 3 qui suit présentera les détails techniques de notre projet, le but est de faire sortir les différentes étapes que nous avons suivies pour arriver à un produit fini et utilisable.

 




%\begin{table}[htp]
%\caption{Confusion Matrix}
%\begin{center}
%\begin{tabular}{|c|c|}
%
%\hline
%
%\hline
%
%\hline
%
%\hline
%
%\end{tabular}
%\end{center}
%\label{default}
%\end{table}%

    %%%%%%%%%%%%%%%%%%%. Chapitre 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage

\chapter{Approche: Construction du modèle machine learning} \label{chap:modelml}
\section{Introduction}

La construction d'un modèle de machine learning est la première grande phase dans l'implémentation de notre solution. Le modèle estimateur (meilleur modèle entrainé en fonction d'un ensemble de données) va être utilisé pour introduire la deuxième phase, celle de la création de l'API \textit{Ortho}. Comme nous avons déjà vu dans l'introduction le diagramme de cas d'utilisation du système machine learning. Nous proposons maintenant un diagramme de classe (figure \ref{MLClassDiag}) qui répond aux différentes exigences que nous avons décrites. Nous tenons à souligner que dans notre diagramme de classe nous ne prenons pas en compte les classes prédéfinies dans les librairies de Python. Par exemple le \textbf{DataFrame} est une classe prédéfinie dans la librairie \textbf{pandas}. Nous l'utilisons dans notre classe \textbf{Gather} mais elle n'est pas définie dans notre diagramme de classe.  \\ 

Dans notre projet, le problème d'apprentissage est un problème de classification parce que notre variable cible est une étiquette discrète et c'est binaire parce qu'on doit décider si oui ou non le patient reste à l'hôpital après une intervention chirurgicale. Si le patient reste, la valeur de la classe est positive (c'est-à-dire 1), s'il ne reste pas ou il est renvoyé, la valeur de la classe est négative (c'est-à-dire 0). Puisque dans la majorité des cas les médecins ne gardent pas les patients à l'hôpital après l'arthroplastie, alors notre ensemble de données est déséquilibré. Plus de 80\% des patients sont retournés à la maison. C'est normal parce que s'ils restent, les coûts sont couverts par l'hôpital. Constatant ce déséquilibre, nous sommes obligés d'appliquer une technique de re-échantillonnage afin d'éviter que notre modèle soit biaisé.  Nous avons opté d'équilibrer notre ensemble de données en appliquant la technique \textit{SMOTE} parce qu'elle a l'avantage d'atténuer le problème de sur-adaptation causé par le sur-échantillonnage aléatoire. Ce dernier est une technique traditionnelle qui a beaucoup d'inconvénients dont nous ne traitons pas dans ce rapport. Si vous voulez avoir plus de connaissances sur ces techniques, vous pouvez lire ces articles \cite{key45, key23, key29}. \\

%Pour développer notre projet, nous avons fait le cycle complet d'un data scientist. Depuis l'obtention des données jusqu'au déploiement d'un système en production. Dans ce chapitre nous allons parcourir toutes les étapes du développement de notre projet. Chaque section représente une étape du projet. Il peut y avoir de petites définitions pour éclairer certains thèmes que nous n'avons pa pu aborder dans le chapitre sur le \textbf{Contexte Théorique}.


Dans les sections qui suivent nous allons voir les différentes étapes d'implémentation de notre modèle machine learning. Nous commençons d'abord par l'obtention des données, pour ensuite continuer avec la visualisation, le nettoyage, la transformation et enfin la modélisation. Ce qui donne quasiment le cycle complet d'un travail de scientifique de données.

\begin{figure}[p]
\includegraphics[width=13cm, height=19cm]{images/MLClassDiag.png}
\caption{Diagramme de classe de notre modèle machine learning}
\label{MLClassDiag}
\end{figure}
\newpage


\section{Collecte de notre ensemble de données}
Une fois qu'on décide d'attaquer un problème de machine learning, la première chose à faire est d'explorer toutes les pistes possibles pour récupérer les données. En effet, les données constituent l'expérience, les exemples qu'on va fournir à notre algorithme de machine learning afin qu'il puisse apprendre et devenir plus performant. D'après la définition de machine learning de Tom Mitchell qu'on a vue à la section \ref{section:ml}, l'expérience $E$ constitue la source des connaissance ou la source des données. Suivant cette définition pour qu'il y ait l'apprentissage, il faut disposer d'une source de données.\\   

Nos données proviennent du département orthopédique de l'université de Seattle, Washington, États-Unis. Elles sont au nombre de 2718 lignes et 92 colonnes. C'est un ensemble de données orthopédique très sensible. Ces données ne sont pas accessibles au public, elles sont hautement confidentielles; pour avoir accès à ces données nous avons signé un contrat de confidentialité dont la clause est de sauvegarder secrètement ces données. En général les bases de données médicales sont des données privées et extrêmement sensibles; d'après la loi sur la protection des informations privées, on ne peut pas accéder à ces données sans l'autorisation des ses propriétaires. \\

Avant de nous donner accès à ces données, on a supprimé toutes les informations personnelles du patient sauf son âge qui est une donnée pertinent pour l'analyse prédictive. Les données personnelles qu'on a supprimées sont entre autres : le nom, le prénom et l'adresse complète. L'objectif est de protéger le plus possible les informations des patients. \\

L'ensemble de données est hébergé sur le serveur de la compagnie pour laquelle je fais mon stage et il est disponible dans un fichier de format \textit{csv}. La compagnie les a obtenues de l'université Seattle, assurément après une entente entre les deux entités. Nous avons eu un accès direct au serveur pour pouvoir télécharger ces données et les utiliser pour construire notre modèle.\\

Nous avons divisé notre ensemble de données en deux parties : un ensemble d'entrainement et un ensemble de test. 80\% de l'ensemble des données est conservé pour la phase d'entrainement et 20\% pour la phase de prédiction. La plus grande difficulté c'est que l'ensemble de données est peu nombreux pour construire un modèle prédictif hautement performant. En ML plus les données sont nombreuses, plus on est capable de construire de bon modèle. \\

Comme vous pouvez remarquer dans la classe diagramme du machine learning (figure \ref{MLClassDiag}), on a implémenté un classe nommant \textit{Gather}. Celle-ci nous permet d'obtenir les données brutes à travers la méthode \textit{read} qui prend en paramètre une chaine qui contient le chemin et nom de fichier (\textit{filename}) et un séparateur (\textbf{sep}). Le fichier contient toutes les données disponibles pour la construction du modèle et le séparateur permet de préciser comment les colonnes sont séparées entre elles, dans notre cas c'est un point virgule (\textit{;}) qui les sépare.\\

Après avoir obtenu ces données, nous devons maintenant les explorer pour avoir des idées claires sur l'ensemble des données. La section suivante est l'étape d'exploration des données, les données obtenues sont transférées à la classe Exploration de données. 

\section{Exploration de notre ensemble de données}

L'analyse exploratoire est une technique permettant de visualiser l'ensemble de données dans le but d'en prendre connaissance. Elles permettent de déterminer les relations qui existent entre les variables de notre ensemble de données.  L'intérêt de la visualisation est de détecter les valeurs aberrantes et vérifier certaines hypothèses avant de choisir notre modèle comme la linéarité de certaines valeurs. Nous faisons cette démarche à l'avance parce que nous voulons obtenir de précieux conseils pour le nettoyage de données et aussi avoir des idées claires pour l'ingénierie des caractéristiques \textbf{(Feature Engineering en anglais)}.  \\

Pour l'analyse exploratoire de notre ensemble données, Tout d'abord, nous commençons par une analyse basique de l'ensemble de nos données. La table \ref{tab:dataset} donne une description de notre ensemble de nos données utilisé pour notre projet et la figure \ref{fig:samplesdataset} donne une vue de notre ensemble de données, leur représentation, et leurs valeurs possibles. Si nous nous référons à notre classe diagramme \ref{MLClassDiag}), c'est l'attribut info de la classe Explore qui nous fournit les informations sur notre dataset. La méthode head nous retourne les cinq premières lignes du jeu de données.

Voyons une brève description de chaque composante de notre ensemble de données après une analyse basique de notre exploration:

\paragraph*{Nombre de variables.} Représente le nombre totale de variables ou colonnes de notre ensemble de données. Les variables prédictives sont importantes pour un modèle de machine learning puisque le modèle les utilise pour prédire des événements. Dans l'annexe A, La table \ref{tab:dt} donne une description de tous les variables de notre ensemble de données. Le nombre total des variables est 92.  La colonne \textbf{Feature name} représente le nom de chaque colonne. La colonne \textbf{datatype} représente le type de données et les colonnes. Et les colonnes \textbf{non-null} et \textbf{null} représentent respectivement le nombre d'échantillons non-nulls et le nombre d'échantillons nulls. Référez-vous à la table \ref{tab:dt}, pour prendre connaissance des nom de colonnes car nous allons les utiliser plus tard. 



\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Nombre de variables & 92 \\
\hline
Nombre d'observations & 2718\\
\hline
Nombre de variables numériques & 87\\
\hline
Nombre de variables discrètes ou catégorielles & 5\\
\hline
Variable cible & Discharge\\
\hline

\end{tabular}
\caption{Exploration basique de notre ensemble de données}
\label{tab:dataset}
\end{table}

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=18cm,angle=90]{images/examplesfeatures.png}
		\caption{Échantillon de notre ensemble de données}
		\label{fig:samplesdataset}
	\end{center}
\end{figure}


\paragraph*{Nombre d'observations.} Représente le nombre d'échantillons \textbf{(samples or rows en anglais)} de notre ensemble de données. 

\paragraph*{ Variables numériques.} Ce sont les variables dont les valeurs sont numériques ou continues dans notre ensemble de données.

\paragraph*{ Variables discrètes.} Ce sont les variables qui viennent sous forme de texte. Plus loin dans la section de la transformation, nous convertirons ces variables en données numériques car la majorité des algorithmes de machine learning ne reconnaissent pas les données discrètes ou textuelles.

\paragraph*{ Variable cible.} C'est l'étiquette (\textbf{label or target en anglais)} de notre ensemble de données. Comme nous avons vu l'étiquette est une variable très importante pour les algorithmes supervisés car elle permet d'évaluer la valeur de sortie d'une décision. Dans notre cas c'est le \textit{Discharge} qui est notre variable cible, elle permet de dire si oui ou non le patient doit rester à l'hôpital pour une réadaptation. \\

\begin{table}[h]
\centering
\begin{tabular}{| l | c |}
\hline
\textbf{Nom d'étiquète} & Discharge \\
\hline
\textbf{Nombre de classes positives} & 2421 \\
\hline
\textbf{Pourcentage de classes positives} & 89.072848 \% \\
\hline
\textbf{Nombre de  classes négatives} & 297 \\
\hline
\textbf{Pourcentage de classes négatives} & 10.927152 \% \\
\hline
\textbf{Nombre total classes} & 2718 \\

\hline
\end{tabular}
\caption{Description de notre variable cible}
\label{tab:target}
\end{table}

La table \ref{tab:target}, quant à elle, c'est la description de notre variable target value. Dans l'ensemble de données, elle s'appelle \textit{Discharge}. On peut remarquer que les classes positives sont significativement plus nombreuses que les classes négatives, ce qui produit un déséquilibre au niveau de l'ensemble de données. Nous avons déjà discuté des classes déséquilibrées dans la sous-section \ref{subsec:imbalancedclasses} et les différentes techniques pour les équilibrer.

\subsection{Corrélation entre nos variables}
Nous terminons la section d'exploration de données avec les corrélations. Ces dernières nous permettent d'examiner les relations entre les variables numériques de notre ensemble de données. La corrélation est une valeur comprise entre -1 et 1 qui représente à quel point deux entités se déplacent à l'unisson. Deux variables sont positivement corrélés lorsque l'une augmente et l'autre augmente aussi. Par exemple, Le BMI d'un patient et son poids\textbf{ (Weight dans le dataset)} sont positivement corrélés, ce qui veut dire le BMI n'augmente que lorsque son poids augmente.\\

Dans la corrélation négative, lorsqu'une variable augmente, l'autre diminue. Par exemple, quand l'âge du patient augmente son poids diminue. Les corrélations proches de -1 ou 1 indiquent une relation forte entre les variables. Les plus proches de 0 indiquent qu'il y a une relation faible. 0 indique qu'il n'y aucune relation.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=19cm, height =7cm,angle=90]{images/correlation.png}
		\caption{Corrélation entre certaines de nos variables}
		\label{fig:correlation}
	\end{center}
\end{figure}

La figure \ref{fig:correlation} nous donne une idée comment nos variables sont corrélées entre elles. Par exemple, le poids \textbf{ (Weight dans notre ensemble de données)} et le \textbf{BMI} ont une corrélation de 0.81 très proche de 1, c'est-à-dire qu'ils sont fortement relationnés. Nous pouvons déduire que les patients qui ont leur BMI plus élevé sont les plus pesants. L'objectif de la corrélation est de gagner en intuition sur les données, ce qui nous aidera tout au long du flux de travail.\\ 

Pour calculer la corrélation, nous avons utilisé la méthode \textbf{corr} de  la classe \textbf{Explore} de notre diagramme de classes. Nous avons passé un peu de style en paramètre pour avoir les couleurs.  \\

L'exploration nous donne une idée générale de notre ensemble de données, elle nous rend capable de poursuivre les autres étapes avec une marge d'erreur très faible. Nous avons vu l'exploration basique et la corrélation entre nos variables. Nous poursuivons donc avec la phase de nettoyage. La prochaine section est consacrée au nettoyage de notre ensemble de données.

\section{Nettoyage de notre ensemble de données}
Une fois l'étape exploratoire des données est terminée et que nous ayons suffisamment de connaissances des données, c'est le temps maintenant de les nettoyer. Le nettoyage des données \textbf{(Data cleaning en anglais)} consiste à éliminer les observations qu'on ne souhaite pas conserver pour la suite de l'analyse prédictive. Ce sont des informations qui peuvent être erronées, inexactes ou sans intérêt pour la phase de modélisation. Les scientifiques de données consacrent habituellement une très grande partie de leur temps au nettoyage des données  parce que même les algorithmes simples peuvent apprendre des informations impressionnantes à partir d'un jeu de données correctement nettoyé. \\

Nous avons suivi les étapes suivantes pour nettoyer notre ensemble de données :

\begin{itemize}
	\item \textbf{Suppression des observations indésirables.} Les observations indésirables (\textbf{Irrelevant observations, en anglais}) sont celles qui ne sont pas pertinentes pour une analyse prédictive c'est-à-dire elles n'ont aucune influence sur la prédiction, on suggère de les supprimer afin que notre modèle ne soit pas biaisé. Dans notre ensemble de données, nous n'avons pas constaté d'observations indésirables parce que chaque observation est un patient qui a fait l'objet d'une décision de retour. Toutes les observation étaient importantes pour notre analyse.
	\item \textbf{Observations en double.} Les observations en double (\textbf{Duplicate observations, en anglais}) surviennent le plus souvent lors de la collecte des informations, plus précisément lorsque les données proviennent de plusieurs endroits différents et qu'on essaye de les combiner pour avoir un seul ensemble de données. Dans notre cas, l'ensemble de données provient d'un seul endroit (université de Seattle), par conséquent il n'y avait pas de doublons. En plus c'est très difficile de constater des doublons dans un jeu de données médical parce que c'est très dangereux d'avoir deux instances pour un même patient.
	\item \textbf{Données manquantes.} Dans la vie réelle, un ensemble de données contient toujours des instances où un élément particulier peut être absent, ceci est dû parce que les administrateurs de base de données, en raison de conception, admettent que certains attributs peuvent être vides ou nulles. Pour des raisons très pratiques, la plupart des algorithmes d'apprentissage ne fonctionnent pas avec des valeurs nulles, il faut, d'une manière ou d'une autre, trouver un moyen de les gérer. Deux stratégies sont couramment recommandées pour traiter les valeurs manquantes:
	\begin{enumerate}
		\item Suppression des observations qui ont des valeurs manquantes \textbf{(Dropping en anglais)}. On élimine toutes les instances qui ont des données vides ou nulles.
		\item Imputation des valeurs manquantes en fonction d'autres observations où toutes les valeurs sont présentes \textbf{(Imputing en anglais)}. Dans l'imputation, il y a plusieurs méthodes de remplissage: On peut remplir en remplaçant les valeurs nulles soit par "\textbf{zéro (0})", soit par la valeur moyenne "\textbf{mean}" ou par la valeur moyenne "\textbf{median}". 
	\end{enumerate}
	Après avoir essayé les deux techniques de traitement des valeurs manquantes, nous nous rendons compte que la stratégie d'\textbf{imputation} nous donne de meilleurs résultats. Pour arriver à cette affirmation, nous avons essayé plusieurs modèles avec leurs différentes évaluations, puis nous comparons les résultats pour décider lequel est meilleur.\\ 
\end{itemize}

En résumé pour nettoyer notre ensemble de données nous avons rempli les valeurs vides et nulles en utilisant la méthode du calcul de la moyenne car elle nous donne de meilleurs résultats  toujours en comparant les différents résultats de chaque méthode en vue de sélectionner le meilleur. Nous avons développé une fonction qui fait automatiquement le choix du meilleur résultat. Nous tenons à souligner que notre ensemble de données a eu beaucoup de valeurs nulles, c'est la deuxième difficulté de notre projet après les classes déséquilibrées. La méthode \textbf{fiilna} de la classe \textbf{Clean} a été utilisée pour remplir les données manquantes de notre ensemble de données. Elle prend en paramètres un jeu de données et la stratégie de remplissage et retourne un jeu de données sans données manquantes. Une fois qu'on a un ensemble de données correctement nettoyé, on peut poursuivre avec la phase de transformation de données.

\section{Transformation de notre ensemble de données}
La transformation de données se repose sur l'ingénierie des caractéristiques \textbf{(Feature Engineering en anglais)} qui est une technique qui consiste à créer de nouvelles variables prédictives à partir de celles qui sont déjà disponibles ou à supprimer des variables impertinentes. La transformation permet soit l'ajout ou la suppression de variables non-étiquetées, mais généralement elle consiste en la création de nouvelles variables surtout quand on dispose d'une petite quantité de variables. La transformation est aussi une tâche précieuse qui permet au scientifique de données d'améliorer la performance d'un modèle puisqu'elle permet de mettre en évidence les informations clés, de se concentrer sur ce qui est important et d'apporter une certaine expertise au domaine. \\

Dans notre projet, nous avons trouvé beaucoup de variables impertinentes. Par exemple, pendant la phase d'exploration nous avons remarqué que le "\textbf{Weight}" est une variable impertinente car elle très corrélée avec le "\textbf{BMI}". La raison est parce que deux variables prédictives ne doivent pas avoir une trop grande corrélation, cela peut biaisé le modèle et nous donne des résultats trompeurs. Pour cette raison, on a décidé de maintenir le \textbf{"BMI"} et supprimer le \textbf{"Weight"}. Du même coup nous nous sommes rendus compte que le \textbf{"RawDX"} et \textbf{"GHOA}" stockent des données similaires sauvegardées dans des formes différentes. Et on a maintenu le \textbf{"RawDX"} et laissé tomber le \textbf{"GHOA"}. La méthode \textbf{drop} de la classe \textbf{Clean} est utilisée pour supprimer les features non pertinents.  

L'ingénierie des caractéristiques est l'étape qui précède la modélisation. Il prend en entrée le jeu de données nettoyé et retourne un jeu de données préparé pour la phase de modélisation. Les algorithmes d'apprentissage prennent en entrée le jeu de données produit par le processus de transformation. \\ 

Pour faire l'ingénierie de caractéristiques nous avons procédé de deux manières: 
\begin{enumerate}[label=\Roman*.]
\item Premièrement, les variables \textbf{Dx1, Dx2, Dx3 } qui sont déjà disponibles dans notre ensemble de données, sont combinés entre elles pour créer un nouvelle variable appelée \textbf{degree\_dx} (le niveau de diagnostique général du patient). Nous soulignons que  \textbf{Dx1, Dx2, Dx3} représentent respectivement: premier niveau de diagnostique, deuxième niveau de diagnostique et troisième niveau de diagnostique du patient. En combinant ces trois diagnostiques, nous créons un diagnostique général du patient. \\
\textbf{Logique de création : } Pour chaque ligne de l'ensemble de données on regroupe les colonnes \textbf{Dx1, Dx2} et \textbf{Dx3} de telle sorte qu'elles suivent les quatre logiques suivantes :
\begin{enumerate}
	\item Si les trois sont présentes (c'est-à-dire si leurs valeurs ne sont pas nulles ou vides), alors la nouvelle variable \textbf{degree\_dx} prend la valeur 3;
	\item  si seulement les diagnostiques \textbf{Dx1} et \textbf{Dx2} sont présentes et la troisième est absente \footnote{Absente veut dire valeur nulle ou vide}, alors \textbf{degree\_dx} prend la valeur 2;
	\item  si seulement \textbf{Dx1} est présente et les deux autres sont absentes, alors \textbf{degree\_dx} prend la valeur 1; 
	\item  si les trois diagnostiques sont absentes, alors \textbf{degree\_dx} prend la valeur 2.
\end{enumerate}
 À la fin de ce processus, nous conservons la variable résultante \textbf{degree\_dx} dans notre ensemble de données et nous supprimons \textbf{Dx1, Dx2} et \textbf{Dx3}.
  
\item Deuxièmement, nous créons une autre nouvelle  variable nommée \textbf{medcond} qui détermine les conditions médicales du patient après une arthroplastie. Nous nous basons sur l'ensemble des variables pré-opératoires du patient qui sont au nombre de 46 colonnes (Les lignes 30 à 76 de la table \ref{tab:dt}). Ces variables déterminent les conditions pré-médicales du patient. Par exemple la ligne 30 représente les quantités d'hémoglobine blanches, la ligne 33  son niveau de glucose et la ligne 56 s'il est obèse. Référez-vous à la table \ref{tab:dt} si vous voulez avoir plus de connaissances sur les préopératoires.\\
\textbf{Logique de création : }  On attribue la valeur 1 à une colonne pré-opératoire lorsqu'on trouve une valeur non-nulle sinon la valeur zéro est attribuée. à chacune des conditions sauf pour les colonnes suivantes :
\begin{itemize}
\item \textbf{[PreOpHgb, PreOpGlucose, pulm circ, other neuro, chronic pulm]} pour lesquelles on attribue la valeur 2.
\item \textbf{[PreOpC, Paralysis, renal failure, liver failure]}  pour lesquelles on attribue la valeur 3.
\end{itemize}
À la fin de ce processus, on fait la somme de toutes les valeurs pré-opératoires, on attribue le résultat à la nouvelle variable \textbf{medcond} et on supprime toutes les colonnes pré-opératoires.
\end{enumerate}

À la fin de l'étape de transformation, deux nouvelles variables sont créées et quarante-huit sont supprimées. La classe \textbf{Transform} de notre diagramme de classe fait toutes les transformations que nous venons de décrire dans cette section. La méthode \textbf{transform} prend en paramètre le jeu de données nettoyé et les attributs à transformer et elle retourne un jeu de données transformé lequel nous allons utiliser comme entrée dans la phase modélisation. 

\section{Modélisation de notre ensemble données}

La modélisation est une technique qui consiste à choisir un algorithme d'apprentissage pour notre ensemble de données. Il consiste à décrire comment choisir le meilleur modèle qui s'adapte à la problématique qu'on étudie et les raisons pour lesquelles le modèle choisi est meilleur. Ici, meilleur modèle ne veut pas dire qu'un algorithme est meilleur qu'un autre mais tout dépend du cas qu'on étudie.\\

Toutes les étapes que nous venons de détailler dans les sections précédentes constituent des étapes préparatoires pour la modélisation. Dans l'étape d'exploration de données, nous nous sommes rendus compte que notre ensemble de données était déséquilibré. Par conséquent, nous ne pouvons pas utiliser les algorithmes de classification simples tel que \textbf{LogisticRegression}, \textbf{TreeDecisionClassifier}, \textbf{LinearClassifier} etc. Nous avons utilisé l'approche des méthodes ensembliste qui consiste à combiner plusieurs classificateurs simples pour donner un classificateur fort comme nous avons vu à la sous-section \ref{subsect:ensemblemethod} . L'image \ref{ensemble} donne une approche générale sur le fonctionnement et la construction des méthodes d'ensemble. \textbf{Data} représente l'ensemble de données d'origine, (\textbf{C1, C2, ... , Cn}) sont les différents algorithmes de classification faibles qu'on combine pour donner en entrée à \textbf{Vote Classifier}. \textbf{Strong Classifier} est l'algorithme de classification fort qu'on obtient. \\

\begin{figure}[h]
	\includegraphics{images/ensemble.png}
	\caption{Approche basée sur la méthodologie des ensemble \cite{key19} }
	\label{ensemble}
\end{figure}

Au chapitre \ref{chap:background} traitant le \textit{Contexte théorique}, nous avons vu comment manipuler les données déséquilibrées en appliquant l'une des différentes techniques de ré-échantillonnage \textbf{(resampling en anglais)} sur l'ensemble de données d'origine. Nous avons vu les avantages à utiliser la technique de \textit{SMOTE} que les autres. Aussi nous avons brièvement examiner une autre approche, à savoir la méthodologie d'ensemble qui est une manière de combiner les algorithmes de classification faibles pour les rendre forts en vue de les approprier à notre ensemble de données déséquilibrés. 

Pour notre projet nous avons entrainé quatre algorithmes ensemblistes comme on peut constater dans notre classe diagramme \ref{MLClassDiag}; deux d'entre eux sont des algorithmes de boosting, un algorithme stacking et le random forest classification. La table \ref{tab:ensemble} présente une comparaison des différents résultats obtenus en entrainant ces quatre types d'algorithmes ensemblistes différents. Pour sélectionner le meilleur algorithme, nous avons développé des processus automatisés en nous basant sur les scores de chaque modèle. Le meilleur modèle va être celui du meilleur score obtenu.     

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|c|c|c|}
		\hline
		\thead{Algorithm} & \multicolumn{2}{c}{\thead{Metrics}} &\\
		\hline
		& \thead{Accurency score} & \thead{Precision} & \thead{Recall} \\
		\hline
		GradientBoostingClassifier & 0.83 & 0.93 & 0.89  \\
		\textbf{AdaBoostClassifier} & \textbf{0.85} & \textbf{0.93} & \textbf{0.90} \\
		StackingClassifier & 0.78 & 0.91 & 0.84 \\
		RandomForestClassifier & 0.82 & 0.91 & 0.90 \\
		\hline
	\end{tabular}
	\caption{Algorithmes ensemblistes utilisés dans notre projet}
	\label{tab:ensemble}
\end{table}

Nous tenons compte aussi des métriques d'évaluation comme "Precision and Recall". Ces derniers nous permettent de déterminer la performance de notre modèle à classifier les classes positives et celles qui sont négatives. Le but de notre projet était d'obtenir la précision en dessus de 90\%. Généralement, dans la pratique une telle précision est acceptable pour pouvoir avoir un modèle performant.\\

La table \ref{tab:ensemble} nous montre les différents résultats obtenus  pour chaque algorithme. Pour faire le choix du bon algorithme, notre système machine learning est capable d'évaluer chaque algorithme de façon indépendante et itérative pour déterminer quel est le modèle estimateur, tout en combinant les différents résultats de chaque métrique de classification. Dans ce cas, c'est l'algorithme \textbf{AdaBoostClassifier} est le modèle estimé meilleur.  \\

Après avoir déterminé le meilleur modèle à partir des fonctions automatisées. La dernière étape qui nous reste à faire est la sérialisation de notre modèle pour son utilisation future soit dans la phase de déploiement. Parce qu'un modèle doit être capable de prédire de nouvelles données (\textbf{online data}). La section qui vient expliquera en détail comment nous avons procédé pour déployer notre modèle c'est-à-dire le rendre accessible à des utilisateurs qui ne connaissent rien en machine learning.  \\ 



%Les méthodes Sequentiels (Boosting) ou les paramètres sont itérativement adaptés à
%produire un meilleur mélange.
 

\chapter{Implémentation de l'outil Ortho} \label{chap:ortho}
\section{Introduction}

Les systèmes modernes de machine learning facilitent la construction d'un système de décision de base. Cette facilité, cependant, est un peu décevante. Construire et déployer un premier système de décision a tendance à bien fonctionner, et les résultats peuvent être assez impressionnants pour les bonnes applications. L'ajout d'un autre système se passe généralement aussi bien. Cependant, d'étranges interactions peuvent commencer à apparaître d'une manière qui serait impossible du point de vue de l'ingénierie logicielle. Changer une partie du système affecte une autre partie du système, même si des tests isolés peuvent suggérer que cela est impossible. \\

Le problème est que les systèmes basés sur le machine learning peuvent avoir des propriétés très subtiles qui sont très différentes des systèmes logiciels plus traditionnels. En partie, cette différence vient du fait que les sorties des systèmes de machine learning ont des comportements beaucoup plus complexes que les composants logiciels typiques. Cela vient aussi en partie du fait de la nature probabiliste des jugements que de tels systèmes sont appelés à faire. \\

Cette complexité et cette subtilité rendent la gestion de ces systèmes plus délicate que la gestion de systèmes logiques traditionnels, bien modularisés, basés sur des micro-services. Les systèmes complexes d'apprentissage profond peuvent évoluer pour montrer des comportements pathologiques «changer quoi que ce soit, tout changer», même s'ils apparaissent superficiellement comme des micro-services bien conçus avec de hauts degrés d'isolation \cite{key7}.\\

Compte tenu de cette complexité qui existe au niveau des applications de machine learning, nous avons décidé de construire un API Ortho pour faire le déploiement de notre modèle, de telle sorte que s'il faut faire un changement dans la phase d'entrainement du modèle, cela n'affectera pas tout le système en général.  

\section{Construction de l'API Ortho}

Les API (Application Program Interfaces) sont des méthodes de communication logicielle développées sur une norme particulière. De nombreuses entreprises ont leurs propres API publics qui résolvent des problèmes spécifiques pour les développeurs. En transmettant à leur API un paramètre en entrée, l'utilisateur peut recevoir une sortie sans avoir besoin de savoir (ou de comprendre) comment la tâche sous-jacente est effectuée. Les API permettent une fonctionnalité multi-plateforme, rendue possible par une norme agnostique de plate-forme, telle que la spécification REST. La beauté des API est qu'elles sont super accessibles - que vous construisiez une application mobile, des appareils IoT, un serveur ou que vous souhaitiez simplement trouver un moyen de communiquer entre vos propres micro-services, les API vous y aident \cite{key8}.\\

Pour communiquer avec notre modèle, nous avons construit un API en Django framework web en suivant l'architecture présentée dans la figure \ref{django}. Voyons un peu cette architecture:
\paragraph{User.} Ce sont les utilisateurs des systèmes développés. Ils utilisent un interface web de Django pour réaliser leurs tâches.
\paragraph{Django.} C'est le moteur web de Django. Il se compose de toutes les librairies internes du framework.
\paragraph{URL. } C'est un fichier de python contenant tous les URLs autorisés par l'application dont nous développons.
\paragraph{View. } C'est la couche metier de toute application Django. Il fait la liaison entre le model et le template. 
\paragraph{Model. } Le modèle est utilisé pour stocker et maintenir les données. C'est le backend où la base de données est définie. Dans notre cas, le model va être le modèle machine learning que nous avons construit.
\paragraph{Template. } Tout ce qui a trait à la présentation. Tout ce que l'utilisateur peut voir. \\

\begin{figure}[h]
	\includegraphics[width=15cm, height=6cm ]{images/ArchitectureDjango.png}
	\caption{Django Framework web architecture \cite{key26}}
	\label{django}
\end{figure} 

\begin{figure}[h]
	\includegraphics[ ]{images/APICD.png}
	\caption{Diagramme de classe de notre Ortho API }
	\label{ortho}
\end{figure} 

%\begin{figure}[h]
%	\includegraphics[ ]{images/ortho.png}
%	\caption{Connexion between our API and machine learning system }
%	\label{ortho}
%\end{figure} 

Notre API prend un formulaire web de Django en entrée et retourne un message de décision. Le framework django fait la validation du formulaire avant d'appeler l'API. Si tous les champs du formulaire sont bien validés et toutes les valeurs obligatoires sont présentes, alors l'API prend le formulaire et fait un pré-traitement des données. S'il trouve des données optionnelles qui sont vides ou nulles, il les remplace par \textbf{NaN} et s'il trouve des champs booléens, il les remplace par \textbf{0} si c'est \textbf{No} et par \textbf{1} si c'est \textbf{Yes}. \\

Le diagramme de classe présenté dans la figure \ref{ortho} permet de répondre à deux des cas d'utilisation que nous avons défini dans le diagramme de cas d'utilisation. Il nous reste encore deux cas d'utilisation à traiter, nous les laissons comme travaux futurs; par contre notre système peut fonctionner normalement dans la pratique sans ces deux fonctionnalités. Voyons brièvement ce que font les deux classes dans ce diagramme de notre API:

\paragraph{Predict. } La classe qui répond à l'exigence \textbf{Predict} défini à l'introduction. Elle trois attributs et quatre méthodes. L'attribut \textbf{model} est le modèle machine learning désérialisé. L'attribut \textbf{args} est un dictionnaire qui stocke la décision rendue par le modèle et un message de prédiction. La méthode \textbf{predicit} calcule la prédiction en fonction des données du formulaire. La méthode \textbf{save} permet de sauvegarder les prédictions après avoir rendu une décision. 
\paragraph{Evaluation. } La classe d'évaluation permet de répondre à l'exigence \textbf{evaluate} défini dans le diagramme de cas d'utilisation à l'introduction générale. Elle contient deux attributs et trois méthodes. L'attribut \textbf{model} obtient le modèle du machine learning construit. L'attribut \textbf{predictions} est une liste de toutes les prédictions qui ont déjà traitées par le modèle. L'opération \textbf{evaluate} permet de d'évaluer le modèle en fonction de l'ensemble de \textbf{prédictions}. L'opération save sauvegarde le résultat de l'évaluation. \\

\begin{figure}[p]
	\includegraphics[ ]{images/ortho.png}
	\caption{Connexion entre notre API et notre système machine learning }
	\label{ortho}
\end{figure} 

%\section{Présentation de notre API}
%
%La table \ref{tab:api} nous présente les détails de notre API. La colonne "\textbf{Attribute or Method}" désigne les différents attributs et les méthodes que constitue notre API. et "\textbf{Comments}" dit ce que fait chaque attribut ou méthode. On précise aussi les différents paramètres de chaque fonction et leur valeur de retour. 
%
%\begin{table}[t]
%	\centering
%	\begin{tabular}{|p{6cm}|p{10cm}|}
%		\hline
%		\thead{Attribute or Method} & \thead{Comments}\\
%		\hline
%		 model & Variable that allows to recover model saved during the training phase  \\
%		 \hline
%		 preprocess\_request(form) & Method to prepare the data for transformation. Its loads the yaml file that contains the set of attributes wich are authorized and converts null values in NaN, Yes in 1 and No in 0. It returns a dcitionnary of attributes and values. \\
%		 \hline
%		 predict(form) & Function that allows prediction of new data. It takes a "form" input parameter and returns a decision message that says whether the patient should be fired or left. The "form" parameter is a django web form\\
%		 \hline
%		save\_prediction(form, y\_pred) & This function saves the new predictions in a database in order to analyze the behavior of the model in the future. It takes as input two parameters (form, y\_pred). The "form" parameter is always the same django form and "y\_pred" is the decision made by the model.\\
%		\hline
%		get\_predictions() & Returns all predictions\\
%		\hline
%	\end{tabular}
%	\caption{Presents details of our API}
%	\label{tab:api}
%\end{table}

La construction de l'API Ortho est un travail d'ingénierie de logiciels, ce qui permet de combiner ensemble deux champs d'études de l'informatique (\textbf{Machine learning et Software Engineering}) décrit dans la classe digramme présenté à la figure \ref{ortho}. Il faut relater que les modèles de machine learning ont besoin d'une couche d'ingénierie de logiciels pour leur rendre accessible aux utilisateurs qui ne comprennent rien en machine learning. Dans la vraie vie, ceux qui utilisent les modèles ne connaissent rien ni en machine learning ni en software engineering. C'est pourquoi dans tout lieu où il y a une équipe de scientifiques de données qui travaillent, il y a aussi une équipe de développement de logiciels.

\newpage

\section{Déploiement de notre système}
Le système que nous développons a eu deux grandes phases de développement comme mentionné dans l'introduction. En plus, nous avons utilisé Django comme framework web qui a des fonctionnalités prédéfinies que nous utilisons dans notre projet. Maintenant nous allons montrer comment nous combinons tous ces morceaux ensemble pour donner un système. Le diagramme de déploiement présenté à la figure \ref{deployment} donne une vue générale des différentes connexions de chaque système. Voyons une brève description de chaque noeud de notre diagramme de déploiement:
\paragraph{Seattle University Data Server. } C'est le serveur de données de l'université Seattle. Ce serveur contient la base de données des patients qui ont subi des arthroplasties aux États-Unis. C'est notre source de données.
\paragraph{Machine Learning System. } C'est notre système ML que nous avons développé dans le chapitre 3. Il contient cinq composantes.
\paragraph{Web Repository. } C'est le dépôt web qui contient notre modèle machine learning.
\paragraph{Deployment System. } Le système de construction de notre API Ortho et l'environnement Django.
\paragraph{Server web. } L'hébergeur du site web Ortho predictions. Le service va être en ligne. 

\begin{figure}[h]
	\includegraphics[width=17cm, height=24cm ]{images/deploymentdiag.png}
	\caption{Diagramme de déploiement de notre système complet}
	\label{deployment}
\end{figure} 

%\newpage
%
%\section{Conclusion}
%Ce chapitre a détaillé tout ce que nous avons fait dans notre projet. C'est la phase de développement de notre projet. Nous avons vu la phase rapportant sur le développement d'un modèle d'apprentissage profond et la phase rapportant sur le génie de logiciel. La phase phase 1 permet la construction et la sérialisation du modèle  et la phase permet d'appliquer les techniques d'ingénierie de logiciel sur le modèle sérialisé.  Le chapitre qui suit va expliquer comment nous avons fait pour évaluer notre modèle de machine learning pour savoir s'il est fiable et répond aux critères d'évaluation escomptés.



\chapter{Évaluation} \label{chap:evaluation}
\section{Évaluation du modèle machine learning}
L'évaluation des algorithmes de machine learning est une partie essentielle de tout projet. Le modèle peut nous donner des résultats satisfaisants lorsqu'il est évalué à l'aide d'une métrique, par exemple \textbf{precision\_score}, mais il peut donner de mauvais résultats lorsqu'il est évalué par rapport à d'autres mesures, telles que \textbf{logarithmic\_loss} ou toute autre mesure de ce type. Évaluer un projet de machine learning est différent d'évaluer les projets de génie logiciels traditionnels. La différence est que pour évaluer de tels projets, il faut tenir compte des différentes théories de machine learning permettant de faire des évaluations sur des modèles. Ce que nous avons déjà dans la sous-section 2.3.1 portant sur la "\textbf{Mesure de performance des algorithmes de classification}" au chapitre "\textbf{Contexte théorique}". \\

Pour évaluer notre modèle, nous avons séparé notre ensemble de données en deux (Train Set et Test Set). Le train set a été utilisé pour construire le modèle et le test set pour faire l'évaluation du modèle. Nous avons séparé le jeu de données comme suit : 80\% pour le train set et 20\% pour le test set. Puisque notre jeu de données était déséquilibré, nous n'avons pas ré-échantillonné (resampling en anglais) notre ensemble de test, parce que les nouvelles données que le modèle va prédire se seront jamais ré-échantillonné. D'autres raisons de ce choix est que si nous ré-échantillonnons l'ensemble de test, il est fort probable que le modèle soit biaisé, ce qui provoquera de fausses prédictions. \\

  \begin{table}[h]
	\centering
	\begin{tabular}{|p{6cm}|p{10cm}|}
		\hline
		 \thead{Model's name} & AdaBoostClassifier\\
		\hline
		  \thead{Precision score} & 0.9291845493562232 \\
		 \hline
		  \thead{Recall score} & 0.9002079002079002\\
		 \hline
		  \thead{Area under Curve} & 0.8511029411764706 \\ 
		 \hline
		  \thead{F1 score} & 0.914466737064414\\
		\hline
	\end{tabular}
	\caption{Résultats de l'évaluation de notre modèle}
	\label{tab:eval}
\end{table}

La table \ref{tab:eval} donne les résultats de l'évaluation de notre modèle sélectionné comme le meilleur. On pourrait constater que cette table est ressemblée à la table de comparaison des différents algorithmes entrainés dans le projet (table \ref{tab:ensemble}). La raison est que pour comparer et sélectionner les différents algorithmes, il faut d'abord évaluer chaque algorithme. C'est comme une évaluation anticipée. Une interprétation sommaire de cette évaluation est que 8\% des prédictions faites par le modèle est erroné et 92\% sont correctement classées. Pour faire cette interpretation, nous nous tenons compte du '' \textbf{Precision score} '' de la table \ref{tab:eval}. 

\section{Evaluation de notre API Ortho}
Pour évaluer notre API, nous avons construit un diagramme de séquence qui montre comment les orthopédistes peuvent utiliser l'API pour prédire un retour. Et l'API à son tour se connecte au repository du machine learning pour charger le modèle sérialisé par le système machine learning. La figure \ref{SequenceDiag} est la classe diagramme qui montre les interactions entre les orthopédistes et ses différents objets impliqués dans le scénario de la prédiction.

Nous allons décrire brièvement les différents messages qui permettent à un orthopédiste de réussir une prédiction.
\begin{enumerate}
\item \textbf{Fill form and press predict.} Quand l'orthopédiste accède à la page de la prédiction, il doit remplir le formulaire et ensuite appuyer sur le bouton Predict pour envoyer le formulaire;
\item \textbf{Validate Form.} Le moteur Django est engagé de faire la validation automatique du formulaire selon les critères qu'on avait précisés dans l'étape de conception du formulaire;
\item \textbf{Form is validated.} Si le formulaire est validé, Django retourne un message que la validation est vraie c'est-à-dire le formulaire est bien rempli;
\item \textbf{Predict decision using the validated form.} Le formulaire rempli est passé à l'API comme paramètre. L'API fait le traitement du formulaire tel que nous avons vu dans la phase de construction de l'API.
\item \textbf{Load model.} L'API charge le modèle qui est hébergé dans le repository de machine learning. Le repository peut être le serveur de la compagnie ou le cloud;
\item \textbf{Model loaded.} L'API utilise le modèle pour faire la prédiction utilisant le formulaire pré-traité et rendu sous forme d'un dataframe (un objet de la classe DataFrame de la librairie Pandas);
\item \textbf{Decision predicted.} L'API rend la décision si le patient doit retourner ou rester à l'hôpital;
\item \textbf{A message with the decision.} Une réponse en format de chaine de caractère est rendu à l'utilisateur dans ce cas l'orthopédiste. Cette réponse est le message de la décision.
\end{enumerate}


 
\begin{figure}[p]
	\includegraphics[width=17cm, height=10cm ]{images/SequenceDiag.png}
	\caption{Diagramme séquence de notre API tool}
	\label{SequenceDiag}
\end{figure} 

\section{Limitations et travaux futurs}
Notre projet est loin d'être exhaustif, il reste beaucoup de choses à faire mais les parties essentielles du projet sont réalisées. Nous pouvons considérer tout ce que nous avons fait jusqu'à présent dans ce projet comme étant une première version du système. Pour le moment, on peut utiliser le système pour faire la prédiction et l'évaluation de notre modèle déployé. C'est-à-dire n'importe quel orthopédiste peut utiliser le système pour prédire si son patient doit rester à l'hôpital et s'il doit retourner chez lui après une arthroplastie. Pour que le médecin orthopédiste puisse faire ces prédictions, il doit, préalablement remplir un formulaire web qui contient les champs nécessaires pour compléter un dossier médical.   \\

Comme travaux futurs, nous prévoyons inclure les modules qui gèrent le monitoring et management du modèle. Et aussi la gestion des sessions puisque tout le monde peut avoir accès au module de prédictions mais les modules monitoring et management sont réservés aux administrateurs du système.    
\newpage

\chapter{Travaux connexes} \label{chap:relatedworks}
Nous n'avons pas trouvé de travaux connexes directement liés à notre domaine c'est-à-dire aucun organisme n'a encore développé de systèmes liés au risque de retour des patients après une arthroplastie. Par contre nous avons trouvé des projets qui ont des points similaires par rapport à notre projet tels que les classes déséquilibrées, la technique SMOTE, le problème de classification et les algorithmes ensemblistes. Aussi nous avons trouvé des APIs conçus pour être utilisés dans le machine learning.\\

MLlib, une bibliothèque de machine learning distribuée de Apache Spark (plateforme populaire adaptée aux tâches d'apprentissage machine itératives) a été présenté par Xiangrui Meng et al. \cite{key34}. La bibliothèque cible les paramètres d'apprentissage à grande échelle qui bénéficient du parallélisme des données ou du parallélisme des modèles pour stocker et exploiter des données ou des modèles. MLlib consiste en des implémentations rapides et évolutives d'algorithmes d'apprentissage standard pour les paramètres d'apprentissage communs, y compris la classification, la régression, le filtrage collaboratif, la mise en grappe et la réduction de la dimensionnalité. Il fournit également une variété de statistiques sous-jacentes, d'algèbre linéaire et de primitives d'optimisation. Écrit dans Scala et utilisant des bibliothèques d'algèbre linéaire (basées sur C ++) sur chaque nœud, MLlib inclut des API Java, Scala et Python, et est publié dans le cadre du projet Spark sous la licence Apache 2.0. MLlib  fournit une API de haut niveau qui exploite l'écosystème riche de Spark pour simplifier le développement de pipelines de machine learning de bout en bout. Dans notre projet, nous avons conçu notre propre API spécifique à nos besoins. \\

MXNet est une bibliothèque de machine learning multi-langages pour faciliter le développement d'algorithmes ML, en particulier pour les réseaux neuronaux profonds. Incorporé dans le langage hôte, il mélange l'expression symbolique déclarative avec le calcul tensoriel impératif pour maximiser l'efficacité et la flexibilité. Il offre une différenciation automatique pour dériver des dégradés. MXNet est efficace en termes de calcul et de mémoire et fonctionne sur divers systèmes hétérogènes, allant des périphériques mobiles aux clusters GPU distribués. La conception de l'API et l'implémentation du système de MXNet ont été réalisés par Tianqi Chen et al. \cite{key35}. Nous avons préféré de développer notre propre API en fonction de nos besoins et notre ensemble de données. En machine learning, il n'y a pas d'API meilleur que d'autres, tout dépend du contexte de développement qu'on se trouve et la situation à laquelle on fait face. Souvent, il vaut mieux de développer ses propres outils spécifiques et adaptés à ce qu'on veut faire. \\

Une étude a été réalisée par Lars Buitinck et al. \cite{key36} sur l'API scikit-learn et de la manière dont elle mappe les concepts et les tâches de machine learning sur les objets et les opérations dans le langage de programmation Python. Ils ont montré comment une API cohérente à travers le paquet rend scikit-learn très utile dans la pratique: expérimenter avec différents algorithmes d'apprentissage est aussi simple que de substituer une nouvelle définition de classe. Grâce à des interfaces de composition telles que Pipelines, Feature Unions et méta-estimateurs, ces blocs de construction simples conduisent à une API puissante et capable d'accomplir une grande variété de tâches d'apprentissage dans une petite quantité de code facile à lire. Par le biais du typage de canard, l'API cohérente conduit à une bibliothèque qui est facilement extensible, et permet aux estimateurs définis par l'utilisateur d'être incorporés dans le flux de travail scikit-learn sans héritage d'objet explicite. Notre API est aussi connecté à l'API  scikit-learn pour donner de bons résultats dans notre projet. Par exemple, notre API doit accéder à notre modèle de machine learning conçu en utilisant les algorithmes classificateurs de la librairie  scikit-learn. Cette techinque produit de bons résultats puisque l'API  scikit-learn a été conçu pour être utilisé de cette manière.\\

Alberto Fernández et al. \cite{key30} ont fourni une analyse expérimentale approfondie qui permettra de déterminer le comportement des différentes approches proposées dans la littérature spécialisée. Tout d'abord, ils ont utilisé des schémas de binarisation, c'est-à-dire, un par rapport à un et un par rapport à tous, afin d'appliquer les approches standard à la résolution de problèmes de classes binaires déséquilibrées. Deuxièmement, ils ont appliqué plusieurs procédures ad hoc qui ont été conçues pour le scénario d'ensembles de données déséquilibrés avec plusieurs classes. Leur étude expérimentale inclut plusieurs algorithmes bien connus de la littérature tels que les arbres de décision, les machines à vecteurs de support et l'apprentissage par instance, avec l'intention d'obtenir des conclusions globales à partir de différents paradigmes de classification. Tandis que dans notre projet nous avons utilisé les algorithmes ensemblistes qui donnent de meilleurs résultats que les algorithmes simples utilisés dans cette étude.\\

Sreejita Ghosh et al.  \cite{key31} ont réalisé un travail dans le domaine biomédical, un taux de détection élevé de maladies éventuellement rares est généralement hautement souhaitable alors que des erreurs dans la classe majoritaire (par exemple des témoins sains) peuvent être plus acceptables. Par conséquent, l'optimisation de la précision prédictive globale est souvent inadaptée. Ils ont analysé un grand ensemble de données de GC / MS urinaires de 829 contrôles et 68 patients souffrant de l'un des trois troubles stéroïdiens innés. Ils ont utilisé 2 algorithmes comparables capables de gérer de grandes quantités de données manquantes. En outre, ils ont comparé différentes stratégies pour traiter les données fortement déséquilibrées, y compris le sous-échantillonnage, le sur-échantillonnage et l'introduction de coûts par classe. Dans cette étude, ils ont utilisé SMOTE pour équilibrer mais ils n'ont pas utilisé les algorithmes ensemblistes qui sont conçus pour être adaptés aux jeux de données déséquilibrés. \\

Rukshan Batuwita and Vasile Palade \cite{key32} ont démontré que les SVM (Support Vector Machines) pourraient produire des résultats sous-optimaux avec des jeux de données déséquilibrés, malgré c'est une technique de machine learning très populaire. Autrement dit, un classificateur SVM formé sur un ensemble de données déséquilibré peut produire des modèles sous-optimaux qui sont biaisés vers la classe majoritaire et ont une faible performance sur la classe minoritaire, comme la plupart des autres paradigmes de classification. Diverses techniques de pré-traitement et d'algorithmique des données ont été proposées dans la littérature pour atténuer ce problème pour les SVM. Il existe des méthodes qui ont utilisé la combinaison des deux méthodes externes et internes pour résoudre le problème de déséquilibre de classe pour les SVM. La méthode hybride de l'ensemble des machines à noyau (HKME) combine un SVM binaire standard et un classificateur SVM à une classe pour résoudre le problème du déséquilibre de classes. La différence avec notre projet, c'est que nous avons utilisé des techniques dédiées pour faire le resampling, non pas des techniques combinées comme proposé dans cet article.\\

Rok Blagus and Lara Lusa \cite{key33}  ont appliqué SMOTE (Synthetic Minority Oversampling TEchnique ) à des données déséquilibrées de classe à haute dimension (à la fois simulées et réelles) et ont également utilisé des résultats théoriques pour expliquer le comportement de SMOTE. Les principales conclusions de leur analyse sont:

\begin{itemize}
\item dans le réglage de faible dimension SMOTE est efficace pour réduire le problème de déséquilibre de classe pour la plupart des classificateurs;

\item SMOTE n'a pratiquement aucun effet sur la plupart des classificateurs formés aux données de grande dimension;

\item lorsque les données sont de haute dimension SMOTE est bénéfique pour les classificateurs k-NN si la sélection de variables est effectuée avant SMOTE;

\item SMOTE n'est pas bénéfique pour les classificateurs d'analyse discriminante, même dans le cadre de basse dimension;

\item sous-échantillonnage ou, pour certains classificateurs, l'ajustement de coupure est préférable à SMOTE pour les tâches de prédiction de classe de grande dimension.

\end{itemize}

Dans cette étude, les auteurs ont décrit les avantages d'utiliser SMOTE mais ils n'ont pas proposé les algorithmes qui fonctionnent mieux avec cette technique. Si vous utilisez SMOTE dans vos projets, vous ne pouvez pas utiliser les algorithmes classiques de machine learning parce que les résultas peuvent être biaisés facilement. Il faut toujours utilisé les classificateurs fort comme les ensemblistes.


  

%Philip K. Chan, Computer Science, Florida Institute of Technology \& Salvatore J. Stolfo, Department of Computer Science, Columbia University \cite{key23} ont travaillé sur un projet de détection de fraude par carte de crédit. Ils ont eu leur dataset déséquilibrés (distributions uniformes dans leur article), 20\% de cas étaient frauduleux contre 80\% non-frauduleux.  Pour rappel, la distribution de notre dataset est présentée à la table \ref{tab:target}, avec 11\% de classes négatives et 89\% de classe positives. Les auteurs ont utilisés juste un  algorithme stacking comme méthodes d'ensemble, tandis que nous, nous avons essayé quatre types d'algorithmes différents en vue de sélectionner le meilleur. Pour le resample(ré-échantillonnage), ils ont utilisé le random over-sampling comme méthode, cette méthode est trop basique pour ces genres de situation car elle ne permet pas de gérer le overfitting. Tandis que nous, nous avons opté pour la technique de boosting qui a beaucoup plus d'avantage, car elle permet de gérer la variance et le biais.\\
%
%Yuchun Tang et al. \cite{key29} ont travaillé sur un projet de detection d'emails spam et ceux qui sont non-spam. Dans leur projet, le dataset est hautement déséquilibré comme montré à la table \ref{tab:Tang}. C'est plus déséquilibré que notre dataset. Il est courant que dans n'importe quel serveur web que les emails spam sont hautement plus nombreux que ceux qui sont non-spam. Ce qui provoque un déséquilibre sérieux dans l'ensemble de données. \\
%
%\begin{table}[h]
%	\centering
%	\begin{tabular}{| p{5cm} |p{2cm} |  p{2cm}|}
%		\hline
%		 & \thead{spam} & \thead{non-spam} \\
%		\hline
%		  training data & 93.96\% & 6.04\% \\
%		 \hline
%		  validation data & 93.09\% & 6.91\% \\
%		 \hline
%		 testing data & 94.99\% & 5.01\%  \\ 
%		 \hline
%	\end{tabular}
%	\caption{Target value of Yuchun Tang et al. project}
%	\label{tab:Tang}
%\end{table}
%
%Comme technique de resampling, le \textbf{Granular Computing} et le \textbf{SVM (\textit{Support Vector Machines})} sont utilisés pour le sous-échantillonnage  (\textbf{\textit{under-sampling en anglais}}) en conservant des échantillons informatifs tout en éliminant les échantillons non pertinents, redondants ou même bruyants. Après le sous-échantillonnage, les données sont nettoyées et, par conséquent, un bon classificateur peut être modélisé pour la classification IP en termes d'efficacité et d'efficience. La technique utilisée dans ce projet a une désavantage très importante, c'est la perte d'informations. Il faut être prudent quand on utilise cette technique, puisqu'on peut perdre des informations pertinentes pour l'entraînement du modèle. Comme précisé dans leur article, les informations supprimées sont non-pertinentes pour le projet; mais généralement, c'est une technique à éviter ou à utiliser minutieusement. \\
%
%Comme algorithmes de classification, ils ont utilisés les quatre algorithmes suivants  \cite{key29}:
%\begin{enumerate}
%\item GSVM-BA algorithm for building a decision tree
%\item The NBTree algorithm for building a decision tree with naïve Bayes classifiers at the leaves
%\item The Logistic algorithm for building a multinomial logistic regression model with a ridge estimator
%\item The SVM algorithm for building a support vector machine
%\end{enumerate}
%
%Le meilleur algorithme obtenu après évaluation était GSVM-BA, avec le score 83\%  \cite{key29}. Tandis que dans notre projet, nous avons obtenu  92\% avec l'algorithme AdaBoostClassifier qui est un algorithme de boosting qui donne de meilleurs résultats que n'importe quel autre algorithme. Puisque ce projet a été développé depuis 2008, c'est sûr à cette époque on ne parlait pas encore des algorithmes de méthodes d'ensemble. \\
%
%Pour conclure le chapitre sur les travaux connexes, nous répétons ce que nous avons dit au commencement de ce chapitre qu'il n'y a pas de projets déjà réalisés sur l'évaluation du risque de retour. Mais comme notre dataset était déséquilibré, nous avons comparé notre projet avec deux autres projets qui ont eu, eux-aussi, leurs datasets déséquilibrés. Nous avons vu les différentes techniques de resampling et les différents algorithmes que les auteurs ont utilisés dans le cadre de leurs projets. Finalement, nous avons comparé leurs techniques avec les nôtres, aussi les différents résultats obtenus après évaluation.  



\newpage

\chapter{Conclusion générale} \label{chap:conclusion}

Nous avons développé un outil qui permet aux orthopédistes américains de déterminer de façon automatique quand un patient doit retourner chez lui après une arthroplastie et quand il doit reste à l'hôpital pour une réadaptation. Le projet était important parce que les compagnies d'assurance de santé américaine ne couvrent pas les frais relatifs à la réadaptation d'un patient qui sont très élevés soit environ \$ 30,000 par réadaptation. Ces coûts doivent être couverts par les hôpitaux et le compagnies d'assurance ne couvrent que les arthroplasties qui sont les opérations. Nous avons dit également, il est très fastidieux pour un médecin de vérifier manuellement un dossier patient compte tenu du volume des informations. Grâce à notre outil les docteurs orthopédistes américains peuvent prédire automatiquement la décision à prendre après une arthroplastie.  \\    

Pour développer cet outil, nous avons démontré comment nous pouvons combiner les techniques de machine learning et celles du software engineering pour aboutir à l'implémentation d'un système complet. Par conséquent, notre projet a été divisé en deux grandes phases qui, elles-mêmes, contiennent plusieurs étapes. Dans la première phase du projet, on a réalisé le cycle de travail complet d'un scientifique de données depuis l'obtention des données jusqu'à la modélisation (étape qui consiste à entrainer différents modèles machine learning en vue  de sélectionner le meilleur qui répond à notre problématique). Cette phase se termine par la sérialisation du modèle estimé meilleur en vue de son utilisation future. Dans la deuxième phase, nous avons vu le déploiement de notre modèle machine learning sérialisé. Cette phase est la conception de notre API Ortho. Ce dernier a été conçu en utilisant le framework web Django que nous avons décrit dans le chapitre 4. \\

Pour chaque phase de notre projet, nous avons construit un diagramme de cas d'utilisation expliquant les différentes exigences que notre système doit satisfaire et nous avons donné une brève explication de chaque cas d'utilisation dans l'introduction générale. Dans les chapitres 3 et 4 nous avons dessiné deux diagrammes de classe, un pour chaque diagramme de cas d'utilisation dans le but d'expliquer les différentes opérations nécessaires pouvant satisfaire les exigences que nous avons décrites. Nous avons vu un diagramme de déploiement permettant de connecter les différents noeuds facilitant le déploiement du système. 

\paragraph{Difficultés rencontrées. } 
Les développements des projets de machine learning peuvent avoir des problèmes différents  l'un de l'autre. Les facteurs qui déclenchent ces problèmes peuvent être liées soit au développement technique du projet, soit à l'administration ou aux interdictions légales. Pour réaliser ce projet de stage, nous n'étions pas exempts de ce problème, nous avons fait face à des difficultés techniques et parfois même administratives comme tout autre projet dans un sens général. La liste suivante donne une vue générale de quelques-unes que nous avons rencontrées :
\begin{itemize}
\item La première difficulté que nous avons eu c'est de combiner le machine learning et le software engineering. Puisqu'au début du stage je n'ai pas eu de connaissances en machine learning;
\item choisir la meilleure technique de ré-échantillonnage (resampling) est une difficulté car aucune technique n'est supérieure à d'autres, tout dépend de la problématique étudiée. Pour choisir le SMOTE, il a fallu qu'on essaye plusieurs techniques et comparer les résultats pour déterminer le meilleur en fonction de notre jeu de données;
\item le choix de l'algorithme était une autre difficulté puisque les algorithmes traditionnels ne fonctionnent pas sur un jeu de données déséquilibré. Même pour choisir un algorithme ensembliste est difficile car chacun a une approche différente l'une de l'autre. Par exemple les algorithmes boosting sont séquentiels tandis que les bagging sont avec remplacement. Donc le choix devient difficile;
\item l'ensemble de données était très petit pour faire l'entraînement des modèles. En machine learning plus votre ensemble de données est petit, plus votre modèle n'est pas fiable. Trouver de nouvelles données est difficile puisque ça doit prendre six mois environ à cause des démarches administratives à entreprendre;
\item les données médicales sont des données très sensibles. Avoir accès à ces données est difficile car c'est interdit par la loi.  
\end{itemize}

\paragraph{Leçons apprises. } 
Puisque nous soutenons que nous avons rencontré des problèmes dans le développement de notre projet, il est primordial de faire ressortir les différentes leçons que nous avons apprises à partir de ces difficultés. Voici deux grandes leçons que j'ai apprises au cours du développement de ce projet : 

\begin{itemize}
\item \textbf{Combinaison de plusieurs spécialités.} Nous avons appris qu'il est possible de combiner plusieurs spécialités en informatique dans la réalisation d'un même projet. Par exemple un API développé en Software engineering peut être utilisé pour connecter à des modèles développés en machine learning. On peut aussi utiliser les diagrammes UML pour décrire un système de machine learning ou n'importe quel autre système en général. De même que les sciences ne sont pas isolées entre elles, de même aussi les spécialités informatiques ne sont pas isolées entre elles;
\item \textbf{Technologies apprises.}  Au commencement de notre stage, nous ne connaissions rien ni en machine learning ni en langage Python. Maintenant nous savons utiliser la majorité des librairies de machine learning développées en Python telles que \textbf{sckit-learn}, \textbf{mlxtend}, \textbf{pandas}, \textbf{matplotlib}, \textbf{numpy}, \textbf{scipy} etc. Nous avons appris aussi à développer des Web API en Django, un framework web en Python. 
\end{itemize}

Notre outil va être déployé sur le serveur web de la compagnie pour laquelle nous faisons le stage (ML+). L'outil va être en ligne et son utilisation sera gratuite pour tous les orthopédistes américains qui veulent uniquement prédire un retour. Mais ceux qui veulent veulent évaluer, suivre et gérer un modèle doivent être d'abord administrateur du système. Pour être administrateur, il faut contacter l'équipe de ML+ et vous aurez un compte qui vous donnera accès; néanmoins certaines conditions peuvent être appliquées tel que un frais unique. Le code de source de notre projet est disponible sur Github et le lien pour y accéder est le suivant : https://github.com/top1986/NewIntershipProject.   


\newpage

\appendix
\chapter{Annexe A}

%\begin{figure}[h]
%	\includegraphics[ ]{images/ortho.png}
%	\caption{A class diagram to explain connexion between our API and machine learning system }
%	\label{cd}
%\end{figure} 




\begin{longtable}{| p{.02\textwidth} | p{.25\textwidth} | p{.13\textwidth} | p{.10\textwidth}  | p{.10\textwidth} | p{.40\textwidth} |  } 
\hline
\textbf{ID} & \textbf{Feature name} & \textbf{datatype} & \textbf{non-null }& \textbf{ null } & \textbf{Comments}\\ \hline

1 &	Side & Categorical & 2718 & 0 & Détermine si c'est à gauche ou à droite du patient a eu son arthroplastie. Deux valeurs possibles : R (pour Right en anglais) et  L (Pour Left en anglais) \\ \hline 
2 &	Procedure & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient  a eu une procédure dans le passé \\ \hline 
3 &	RawDx & Categorical & 2718 & 0 & Diagnostique brute du patient  \\ \hline 
4 &	PriorContralateral & Boolean & 2703 & 15& Colonne de type booléen qui indique si le patient souffre  de \textit{PriorContralateral} \\ \hline 
5 &	TSA & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{TSA} \\ \hline 
6 &	RTSA & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{RTSA} \\ \hline 
7 &	Tendon Transfer & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{Tendon Transfer} \\ \hline 
8 &	Dx1 & Categorical & 2718 & 0 & Représente diagnostique 1 \\ \hline 
9 &	Dx2 & Categorical & 176 & 2542 & Représente diagnostique 2 \\ \hline 
10 &	Dx3 & Categorical & 2 & 2716 & Représente diagnostique 3 \\ \hline 
11 &	GHOA & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{GHOA} \\ \hline 
12 &	AVN & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{AVN} \\ \hline 
13 &	RCT & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{RCT} \\ \hline 
14 &	FailedRCR & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{FailedRCR} \\ \hline 
15 &	ComboRCT & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{ComboRCT} \\ \hline 
16 &	RCTA & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{RCTA} \\ \hline 
17 &	IA & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{IA} \\ \hline 
18 &	PTA & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{PTA} \\ \hline 
19 &	PHFx & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{PHFx} \\ \hline 
20 &	PHFxSequelae & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{PHFxSequelae} \\ \hline 
21 &	Other & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient souffre  de \textit{Other} \\ \hline 
22 &	AGE\_AT\_ADMIT & Numerical & 2718 & 0 & L'âge du patient \\ \hline 
23 &	ASA\_SCORE & Numerical & 2716 & 2 & Nous n’avons pas trouvé l’explication de ces colonnes \\ \hline 
24 &	Gender & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient  est un \textit{Gender} \\ \hline 
25 &	Female & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient  est un \textit{Female} \\ \hline 
26 &	Discharge & Boolean & 2718 & 0& Colonne de type booléen qui indique si le patient  doit retourner chez lui \\ \hline 
27 &	Height & Numerical & 2293 & 425 & La hauteur du patient \\ \hline 
28 &	Weight & Numerical & 2301 & 417 & Le poids du patient \\ \hline 
29 &	BMI & Numerical & 2283 & 435 & L'indice de masse corporelle (IMC)- BMI en anglais \\ \hline 
30 &	PreOpHgb & Numerical & 272 & 2446 & Hémoglobines blanches du patient \\ \hline 
31 &	PreOpCr & Numerical & 255 & 2463 & Conditions pré-opératoires du patient \\ \hline 
32 &	PreOpALC & Numerical & 157 & 2561 &  Conditions pré-opératoires du patient \\ \hline 
33 &	PreOpGlucose & Numerical & 253 & 2465 & Glucose du patient \\ \hline 
34 &	arrhythmias & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{arrhythmias} \\ \hline 
35 &	valvular & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{valvular} \\ \hline 
36 &	pulm circ & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{pulm circ} \\ \hline 
37 &	PVD & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{PVD} \\ \hline 
38 &	HTNw/oCx & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{HTNw/oCx} \\ \hline 
39 &	HTNw/Cx & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{HTNw/Cx} \\ \hline 
40 &	Paralysis & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{Paralysis} \\ \hline 
41 &	other neuro & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{other neuro} \\ \hline 
42 &	chronic pulm & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{chronic pulm} \\ \hline 
43 &	DMw/oCx & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{DMw/oCx} \\ \hline 
44 &	DMw/Cx & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{DMw/Cx} \\ \hline 
45 &	hypothyroid & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{hypothyroid} \\ \hline 
46 &	renal failure & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{renal failure} \\ \hline 
47 &	liver failure & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{liver failure} \\ \hline 
48 &	PUD & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{PUD} \\ \hline 
49 &	AIDS & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{AIDS} \\ \hline 
50 &	L0oma & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{L0oma} \\ \hline 
51 &	Mets & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{Mets} \\ \hline 
52 &	solidCaw/oMets & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{solidCaw/oMets} \\ \hline 
53 &	RA/CVD & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{RA/CVD} \\ \hline 
54 &	coagulopathy & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{coagulopathy} \\ \hline 
55 &	obesity & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{obesity} \\ \hline 
56 &	weightLoss & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{weightLoss} \\ \hline 
57 &	fluidElectrolyte & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{fluidElectrolyte} \\ \hline 
58 &	bloodLossAnemia & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{bloodLossAnemia} \\ \hline 
59 &	DeficiencyAnemia & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{DeficiencyAnemia} \\ \hline 
60 &	EtOH & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{EtOH} \\ \hline 
61 &	Drugs & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{Drugs} \\ \hline 
62 &	Psychosis & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{Psychosis} \\ \hline 
63 &	Depression & Boolean & 2092 & 626& Colonne de type booléen qui indique si le patient souffre  de \textit{Depression} \\ \hline 
64 &	Plavix & Numerical & 2482 & 236 & Nous n’avons pas trouvé l’explication de ces colonnes \\ \hline 
65 &	Dabigatran & Boolean & 2482 & 236& Colonne de type booléen qui indique si le patient souffre  de \textit{Dabigatran} \\ \hline 
66 &	Enoxaparin & Numerical & 2482 & 236 & Nous n’avons pas trouvé l’explication de ces colonnes \\ \hline 
67 &	Rivaroxaban & Numerical & 2482 & 236 & Nous n’avons pas trouvé l’explication de ces colonnes \\ \hline 
68 &	Warfarin & Numerical & 2482 & 236 & Nous n’avons pas trouvé l’explication de ces colonnes \\ \hline 
69 &	PreOpNarcotic & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{PreOpNarcotic} \\ \hline 
70 &	PreOpBloodThinner & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{PreOpBloodThinner} \\ \hline 
71 &	PreOpSteroids & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{PreOpSteroids} \\ \hline 
72 &	PreOpInsulin & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{PreOpInsulin} \\ \hline 
73 &	PreOpDMMeds & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{PreOpDMMeds} \\ \hline 
74 &	Oxycodone & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Oxycodone} \\ \hline 
75 &	Hydrocodone & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Hydrocodone} \\ \hline 
76 &	Vicodin & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Vicodin} \\ \hline 
77 &	percocet & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{percocet} \\ \hline 
78 &	Oxycontin & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Oxycontin} \\ \hline 
79 &	Dilaudid & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Dilaudid} \\ \hline 
80 &	Aspirin & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Aspirin} \\ \hline 
81 &	Warfarin.1 & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Warfarin.1} \\ \hline 
82 &	Coumadin & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Coumadin} \\ \hline 
83 &	Plavix.1 & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Plavix.1} \\ \hline 
84 &	Clopidogrel & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Clopidogrel} \\ \hline 
85 &	Prednisone & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{Prednisone} \\ \hline 
86 &	insulin & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{insulin} \\ \hline 
87 &	humulin & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{humulin} \\ \hline 
88 &	lantus & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{lantus} \\ \hline 
89 &	lispro & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{lispro} \\ \hline 
90 &	aspart & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{aspart} \\ \hline 
91 &	metformin & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{metformin} \\ \hline 
92 &	glipizide & Boolean & 900 & 1818& Colonne de type booléen qui indique si le patient souffre  de \textit{glipizide} \\ \hline



\caption{Notre ensemble de données complet}
\label{tab:dt}
\end{longtable}

\newpage
\addcontentsline{toc}{chapter}{Bibliography}

\bibliographystyle{abbrv}
\bibliography{bibliography} 	
\end{document}
